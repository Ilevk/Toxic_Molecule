{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/heartcored98/Standalone-DeepLearning-Chemistry/blob/master/Lec05/Lec05_lipo_graph_gcn_prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install miniconda and rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!wget https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning-Chemistry/master/Lec04/utils.py -O utils.py\\n!mkdir results\\n!mkdir images\\n\\n!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\\n!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\\n!time conda install -q -y -c conda-forge rdkit\\n\\nimport sys\\nimport os\\nsys.path.append('/usr/local/lib/python3.7/site-packages/')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!wget https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning-Chemistry/master/Lec04/utils.py -O utils.py\n",
    "!mkdir results\n",
    "!mkdir images\n",
    "\n",
    "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "!time conda install -q -y -c conda-forge rdkit\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Predict Lipophilicity Property Using GCN\n",
    "=====================\n",
    "\n",
    "앞선 실습에서는 SMILES와 CNN 아키텍쳐를 활용한 lipophilicity 예측을 연습해보았습니다. \n",
    "이번 시간에는 분자를 Graph로 표현하여 각 원자의 벡터와 원자 간의 연결 상태를 input으로 다룰 수 있는 GCN을 구현해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset과 DataLoader 준비하기  \n",
    "\n",
    "Graph 형태로 데이터를 준비하기 위해서는 임의 분자의 `SMILES` 문자열이 들어왔을 때 이를 `Rdkit` 분자 오브젝트로 바꾸고 각 원자들의 node feature vector와 adjacency matrix를 만들 필요가 있습니다. 이후에는 custom dataset을 만들어 노드 벡터들이 담긴 X, 연결 상태가 담긴 A, Lipophilicity 값 y를 반환해주면 됩니다.  \n",
    "\n",
    "\n",
    "### 1.1 데이터 준비하기\n",
    "앞선 예제에서 사용했던 것처럼 `Lipophilicity.csv`를 다운 받은 후 train, val, test로 분할해줍니다.  \n",
    "GCN의 경우 문자열의 vocabulary set을 만드는 대신 사용할 원자들의 집합을 직접 정하는 방식으로 대체합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-09 18:49:24,188\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-03-09 18:49:24,192\tINFO resource_spec.py:216 -- Starting Ray with 12.65 GiB memory available for workers and up to 6.32 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import ray\n",
    "ray.init(num_cpus=4)\n",
    "# !wget -q \"http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\" -O Lipophilicity.csv\n",
    "\n",
    "# import pandas as pd\n",
    "import modin.pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "# def get_splitted_lipo_dataset(ratios=[0.8, 0.1, 0.1], seed=123):\n",
    "\n",
    "#     raw_data = pd.read_csv('Lipophilicity.csv') # Open original dataset\n",
    "#     smiles = raw_data['smiles']\n",
    "        \n",
    "#     train_val, test = train_test_split(raw_data, test_size=ratios[2], random_state=seed)\n",
    "#     train, val = train_test_split(train_val, test_size=ratios[1]/(ratios[0]+ratios[1]), random_state=seed)\n",
    "    \n",
    "#     return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = join('data', 'train_.csv')\n",
    "valid_path = join('data', 'valid_.csv')\n",
    "total_path = join('data', 'train.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_valid = pd.read_csv(valid_path)\n",
    "total = pd.read_csv(total_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>ecfp_0</th>\n",
       "      <th>ecfp_1</th>\n",
       "      <th>ecfp_2</th>\n",
       "      <th>ecfp_3</th>\n",
       "      <th>ecfp_4</th>\n",
       "      <th>ecfp_5</th>\n",
       "      <th>ecfp_6</th>\n",
       "      <th>ecfp_7</th>\n",
       "      <th>...</th>\n",
       "      <th>ptfp_1019</th>\n",
       "      <th>ptfp_1020</th>\n",
       "      <th>ptfp_1021</th>\n",
       "      <th>ptfp_1022</th>\n",
       "      <th>ptfp_1023</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>clogp</th>\n",
       "      <th>sa_score</th>\n",
       "      <th>qed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5147</td>\n",
       "      <td>CNC(=O)c1ncn2c1COc3c(CCN4CCN(CC4)c5cccc6nc(C)c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>482.588</td>\n",
       "      <td>3.34562</td>\n",
       "      <td>2.842929</td>\n",
       "      <td>0.470342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5243</td>\n",
       "      <td>CN(C1CCN(Cc2ccc(cc2)C(F)(F)F)CC1)C(=O)Cc3ccc(c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>486.531</td>\n",
       "      <td>3.91350</td>\n",
       "      <td>2.420206</td>\n",
       "      <td>0.581325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4735</td>\n",
       "      <td>COc1cc(ccc1n2cnc(C)c2)c3cn(CC(=O)N(C4CCCCC4)c5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>470.577</td>\n",
       "      <td>4.81372</td>\n",
       "      <td>2.705274</td>\n",
       "      <td>0.387214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6527</td>\n",
       "      <td>CCOC(=O)[C@@H]1CC[C@@H](CC1)N2CC(C2)NC(=O)CNc3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>545.584</td>\n",
       "      <td>2.19700</td>\n",
       "      <td>3.012426</td>\n",
       "      <td>0.484449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5803</td>\n",
       "      <td>COC1(CCOCC1)c2ccc(NC(=O)C3=CC(=O)c4cc(F)cc(c4O...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>506.534</td>\n",
       "      <td>4.24884</td>\n",
       "      <td>3.187803</td>\n",
       "      <td>0.431104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows x 3079 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             SMILES  ecfp_0  \\\n",
       "0        5147  CNC(=O)c1ncn2c1COc3c(CCN4CCN(CC4)c5cccc6nc(C)c...       0   \n",
       "1        5243  CN(C1CCN(Cc2ccc(cc2)C(F)(F)F)CC1)C(=O)Cc3ccc(c...       0   \n",
       "2        4735  COc1cc(ccc1n2cnc(C)c2)c3cn(CC(=O)N(C4CCCCC4)c5...       0   \n",
       "3        6527  CCOC(=O)[C@@H]1CC[C@@H](CC1)N2CC(C2)NC(=O)CNc3...       0   \n",
       "4        5803  COC1(CCOCC1)c2ccc(NC(=O)C3=CC(=O)c4cc(F)cc(c4O...       0   \n",
       "\n",
       "   ecfp_1  ecfp_2  ecfp_3  ecfp_4  ecfp_5  ecfp_6  ecfp_7  ...  ptfp_1019  \\\n",
       "0       0       0       0       0       0       0       0  ...          1   \n",
       "1       0       0       0       0       0       0       0  ...          0   \n",
       "2       0       1       0       1       0       0       0  ...          0   \n",
       "3       0       0       0       0       0       0       0  ...          1   \n",
       "4       0       0       0       0       0       0       0  ...          1   \n",
       "\n",
       "   ptfp_1020  ptfp_1021  ptfp_1022  ptfp_1023    MolWt    clogp  sa_score  \\\n",
       "0          0          1          1          1  482.588  3.34562  2.842929   \n",
       "1          0          1          1          0  486.531  3.91350  2.420206   \n",
       "2          0          1          1          0  470.577  4.81372  2.705274   \n",
       "3          0          1          1          0  545.584  2.19700  3.012426   \n",
       "4          0          1          1          0  506.534  4.24884  3.187803   \n",
       "\n",
       "        qed  label  \n",
       "0  0.470342      0  \n",
       "1  0.581325      0  \n",
       "2  0.387214      0  \n",
       "3  0.484449      1  \n",
       "4  0.431104      1  \n",
       "\n",
       "[5 rows x 3079 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets = get_splitted_lipo_dataset()\n",
    "smiles = df_train\n",
    "smiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_atom_symbols(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    symbols = set([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return symbols\n",
    "\n",
    "def get_num_atom(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    num_atom = len([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return num_atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_SYMBOLS = list(set.union(*total['SMILES'].apply(lambda x: get_unique_atom_symbols(x)).values))\n",
    "MAX_LEN = max(total['SMILES'].apply(lambda x: get_num_atom(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mol2Graph 구현하기  \n",
    "\n",
    "임의의 `SMILES`가 입력되었을 때 이를 분자로 바꿔봅시다. \n",
    "\n",
    "- 노드 행렬 X를 만들 때 원자의 종류뿐만 아니라 Degree, 붙어 있는 수소 원자의 수, Valence, 아로마틱 여부 등을 함께 넣어줌으로써 추가적인 정보를 제공합니다. \n",
    "\n",
    "- 인접 행렬 A를 만들 때는 `Rdkit`에서 기본적으로 계산해주는 행렬에서 대각 성분에 1을 더함으로써 이후 GCN이 이루어질 때 본인의 노드 정보도 함께 추가될 수 있도록 합니다. \n",
    "\n",
    "- 추가적으로 각 분자들은 임의의 원자 개수를 가지고 있으므로 미리 최대 원자 수를 설정해놓고(본 예시에서는 70개) X, A 행렬을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST_SYMBOLS = ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "#             'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "#             'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "#             'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']\n",
    "\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(char_to_ix(atom.GetSymbol(), LIST_SYMBOLS) +\n",
    "                    char_to_ix(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    char_to_ix(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(int(atom.GetIsAromatic()), [0, 1]))    # (40, 6, 5, 6, 2)\n",
    "\n",
    "\n",
    "def char_to_ix(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        return [0] # Unknown Atom Token\n",
    "    return [allowable_set.index(x)+1]\n",
    "\n",
    "\n",
    "def mol2graph(smi, MAX_LEN):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "    X = np.zeros((MAX_LEN, 5), dtype=np.uint8)\n",
    "    A = np.zeros((MAX_LEN, MAX_LEN), dtype=np.uint8)\n",
    "\n",
    "    temp_A = Chem.rdmolops.GetAdjacencyMatrix(mol).astype(np.uint8, copy=False)[:MAX_LEN, :MAX_LEN]\n",
    "    num_atom = temp_A.shape[0]\n",
    "    A[:num_atom, :num_atom] = temp_A + np.eye(temp_A.shape[0], dtype=np.uint8)\n",
    "    \n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        feature = atom_feature(atom)\n",
    "        X[i, :] = feature\n",
    "        if i + 1 >= num_atom: break\n",
    "            \n",
    "    return X, A\n",
    "\n",
    "smiles = \"O=C(NCc1ccccn1)c2ccc(Oc3ccccc3)cc2\"\n",
    "X, A = mol2graph(smiles, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class gcnDataset(Dataset):\n",
    "    def __init__(self, df, max_len=120):\n",
    "        self.smiles = df[\"SMILES\"]\n",
    "        self.exp = df[\"label\"].values\n",
    "                \n",
    "        list_X = list()\n",
    "        list_A = list()\n",
    "        for i, smiles in enumerate(self.smiles):\n",
    "            X, A = mol2graph(smiles, max_len)\n",
    "            list_X.append(X)\n",
    "            list_A.append(A)\n",
    "            \n",
    "        self.X = np.array(list_X, dtype=np.uint8)\n",
    "        self.A = np.array(list_A, dtype=np.uint8)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.A[index], self.exp[index]\n",
    "    \n",
    "sample_dataset = gcnDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.gcnDataset at 0x7ff54a89c5c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction\n",
    "\n",
    "Vanila GCN 기반의 Lipophilicity 예측 아키텍쳐를 구현하여 봅시다. 이를 위해 크게 4가지의 Module을 구현하고 사용합니다.  \n",
    "\n",
    "- **BN1d** X 행렬에 대해서 각 노드 피처 벡터들을 Batch Normalization할 수 있는 module입니다.  \n",
    "- **GConv** X, A를 입력 받아서 인접한 노드들의 정보를 바탕으로 각 노드 벡터를 업데이트하는 module입니다.  \n",
    "- **Readout** GConv를 거친 노드 벡터들로부터 invariant한 molecular vector representation을 만들기 위한 pooling module입니다.  \n",
    "- **GCNNet** 노드 행렬 X를 embedding matrix로 변환한 후 `GConv` 모듈을 통과시키고 `Readout` 모듈로 molvec을 만든 후 이로부터 lipophilicity를 예측하는 module입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = self.dp(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = BN1d(output_dim, use_bn)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        x = self.fc(X)\n",
    "        x = torch.matmul(A, x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.relu(x)\n",
    "        return x, A\n",
    "        \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
    "\n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = torch.mean(molvec, dim=1)\n",
    "        return molvec\n",
    "    \n",
    "\n",
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        # Create Atom Element embedding layer\n",
    "        self.embedding = self.create_emb_layer([args.vocab_size, args.degree_size,\n",
    "                                                args.numH_size, args.valence_size,\n",
    "                                                args.isarom_size],  args.emb_train)    \n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        for i in range(args.n_layer):\n",
    "            self.gcn_layers.append(GConv(args.in_dim if i==0 else args.out_dim, args.out_dim, args.use_bn))\n",
    "                                   \n",
    "        self.readout = Readout(args.out_dim, args.molvec_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.molvec_dim, args.molvec_dim//2)\n",
    "        self.fc2 = nn.Linear(args.molvec_dim//2, args.molvec_dim//2)\n",
    "        self.fc3 = nn.Linear(args.molvec_dim//2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def create_emb_layer(self, list_vocab_size, emb_train=False):\n",
    "        list_emb_layer = nn.ModuleList()\n",
    "        for i, vocab_size in enumerate(list_vocab_size):\n",
    "            vocab_size += 1\n",
    "            emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "            weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "            for i in range(vocab_size):\n",
    "                weight_matrix[i][i] = 1\n",
    "            emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "            emb_layer.weight.requires_grad = emb_train\n",
    "            list_emb_layer.append(emb_layer)\n",
    "        return list_emb_layer\n",
    "\n",
    "    def _embed(self, x):\n",
    "        list_embed = list()\n",
    "        for i in range(5):\n",
    "            list_embed.append(self.embedding[i](x[:, :, i]))\n",
    "        x = torch.cat(list_embed, 2)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        A = A.float()\n",
    "        x = self._embed(x)   \n",
    "        \n",
    "        for i, module in enumerate(self.gcn_layers):\n",
    "            x, A = module(x, A)\n",
    "        x = self.readout(x)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dp(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dp(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return torch.squeeze(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train, Validation, Test\n",
    "\n",
    "Data, Model, Loss, Optimization을 모두 같이 사용하여 봅시다. Epoch 별로 train과 validation, test가 이루어질 수 있게 함수를 나누었습니다. 이 때 `DataLoader`로부터 X, A, y를 받은 후 `model(X,A)`를 수행하여 `pred_y`를 구한다는 점이 CNN 실습과 다릅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, args, **kwargs):\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X, A)\n",
    "        \n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "        \n",
    "        cnt_iter += 1\n",
    "    \n",
    "    f1 = f1_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    accuracy = accuracy_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return model, list_train_loss, (f1, accuracy, list_y, list_pred_y)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, args):\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, dataloader, args, criterion, **kwargs):\n",
    "\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "\n",
    "    f1 = f1_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    accuracy = accuracy_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return f1, accuracy, std, list_y, list_pred_y\n",
    "\n",
    "\n",
    "def experiment(cnt_exp, partition, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    model = GCNNet(args)    \n",
    "    model.to(args.device)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    args.best_valid_loss = 1000\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_losses, train_results = train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
    "#         val_loss = validate(model, partition['val'], criterion, args)\n",
    "        f1_score, accuracy, std, true_y, pred_y = test(model, partition['val'], args, criterion, **{'epoch':epoch})\n",
    "        \n",
    "        list_train_loss += train_losses\n",
    "#         list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "#         list_mae.append({'epoch':epoch, 'f1_score':mae})\n",
    "#         list_std.append({'epoch':epoch, 'std':std})\n",
    "        train_loss = log_loss(train_results[-2], train_results[-1])\n",
    "        valid_loss = log_loss(true_y, pred_y)\n",
    "        \n",
    "        print('[Exp {:2}] Epoch {:2}'.format(cnt_exp,\n",
    "                                               epoch))\n",
    "    \n",
    "        print('Train f1: {:2.3f}, Valid f1: {:2.3f}, Train acc: {:2.3f}, Valid acc: {:2.3f}, Train loss: {:2.3f}, Valid loss: {:2.3f}'.format(\n",
    "                                                    train_results[0],\n",
    "                                                    f1_score, \n",
    "                                                    train_results[1],\n",
    "                                                    accuracy,\n",
    "                                                    train_loss,\n",
    "                                                    valid_loss))\n",
    "        if args.best_valid_loss > valid_loss or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_f1_score = f1_score\n",
    "            args.best_accuracy = accuracy\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "            args.best_valid_loss = valid_loss\n",
    "            print('-------------------------------\\nBest Valid: got f1_score: {:2.3f}, accuracy: {:2.3f}, at epoch {:2}, loss: {:2.3f}\\n------------------------------- '.format(f1_score, \n",
    "                                                       accuracy, \n",
    "                                                       epoch,\n",
    "                                                       valid_loss))\n",
    "            \n",
    "        \n",
    "    # End of experiments\n",
    "    te = time.time()\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "#     args.val_losses = list_val_loss\n",
    "#     args.maes = list_mae\n",
    "#     args.stds = list_std\n",
    "\n",
    "    return model, args "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment\n",
    "\n",
    "실험을 진행해봅시다. 이때 Embedding, Model Architecture, Optimizer, Training Configuration을 설정할 필요가 있습니다.  \n",
    "첫번째 실험으로 Learning Rate와 N Layer를 바꿔가면서 실험해보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp  0] Epoch  0\n",
      "Train f1: 0.689, Valid f1: 0.700, Train acc: 0.604, Valid acc: 0.554, Train loss: 0.662, Valid loss: 0.671\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.700, accuracy: 0.554, at epoch  0, loss: 0.671\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  1\n",
      "Train f1: 0.687, Valid f1: 0.719, Train acc: 0.635, Valid acc: 0.659, Train loss: 0.642, Valid loss: 0.622\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.719, accuracy: 0.659, at epoch  1, loss: 0.622\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  2\n",
      "Train f1: 0.709, Valid f1: 0.661, Train acc: 0.661, Valid acc: 0.672, Train loss: 0.619, Valid loss: 0.607\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.661, accuracy: 0.672, at epoch  2, loss: 0.607\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  3\n",
      "Train f1: 0.718, Valid f1: 0.656, Train acc: 0.670, Valid acc: 0.664, Train loss: 0.611, Valid loss: 0.612\n",
      "[Exp  0] Epoch  4\n",
      "Train f1: 0.716, Valid f1: 0.608, Train acc: 0.674, Valid acc: 0.666, Train loss: 0.609, Valid loss: 0.614\n",
      "[Exp  0] Epoch  5\n",
      "Train f1: 0.715, Valid f1: 0.702, Train acc: 0.682, Valid acc: 0.693, Train loss: 0.597, Valid loss: 0.588\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.702, accuracy: 0.693, at epoch  5, loss: 0.588\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  6\n",
      "Train f1: 0.730, Valid f1: 0.726, Train acc: 0.683, Valid acc: 0.689, Train loss: 0.600, Valid loss: 0.581\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.726, accuracy: 0.689, at epoch  6, loss: 0.581\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  7\n",
      "Train f1: 0.728, Valid f1: 0.713, Train acc: 0.688, Valid acc: 0.585, Train loss: 0.595, Valid loss: 0.926\n",
      "[Exp  0] Epoch  8\n",
      "Train f1: 0.721, Valid f1: 0.681, Train acc: 0.683, Valid acc: 0.680, Train loss: 0.599, Valid loss: 0.589\n",
      "[Exp  0] Epoch  9\n",
      "Train f1: 0.733, Valid f1: 0.383, Train acc: 0.689, Valid acc: 0.587, Train loss: 0.585, Valid loss: 0.729\n",
      "[Exp  0] Epoch 10\n",
      "Train f1: 0.725, Valid f1: 0.748, Train acc: 0.690, Valid acc: 0.703, Train loss: 0.587, Valid loss: 0.566\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.748, accuracy: 0.703, at epoch 10, loss: 0.566\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 11\n",
      "Train f1: 0.743, Valid f1: 0.699, Train acc: 0.703, Valid acc: 0.698, Train loss: 0.577, Valid loss: 0.595\n",
      "[Exp  0] Epoch 12\n",
      "Train f1: 0.733, Valid f1: 0.737, Train acc: 0.690, Valid acc: 0.712, Train loss: 0.580, Valid loss: 0.555\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.737, accuracy: 0.712, at epoch 12, loss: 0.555\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 13\n",
      "Train f1: 0.739, Valid f1: 0.729, Train acc: 0.703, Valid acc: 0.710, Train loss: 0.567, Valid loss: 0.565\n",
      "[Exp  0] Epoch 14\n",
      "Train f1: 0.748, Valid f1: 0.759, Train acc: 0.708, Valid acc: 0.717, Train loss: 0.566, Valid loss: 0.550\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.759, accuracy: 0.717, at epoch 14, loss: 0.550\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 15\n",
      "Train f1: 0.744, Valid f1: 0.633, Train acc: 0.708, Valid acc: 0.678, Train loss: 0.565, Valid loss: 0.583\n",
      "[Exp  0] Epoch 16\n",
      "Train f1: 0.748, Valid f1: 0.697, Train acc: 0.708, Valid acc: 0.697, Train loss: 0.562, Valid loss: 0.565\n",
      "[Exp  0] Epoch 17\n",
      "Train f1: 0.752, Valid f1: 0.710, Train acc: 0.711, Valid acc: 0.695, Train loss: 0.563, Valid loss: 0.555\n",
      "[Exp  0] Epoch 18\n",
      "Train f1: 0.761, Valid f1: 0.713, Train acc: 0.716, Valid acc: 0.717, Train loss: 0.557, Valid loss: 0.553\n",
      "[Exp  0] Epoch 19\n",
      "Train f1: 0.743, Valid f1: 0.742, Train acc: 0.705, Valid acc: 0.715, Train loss: 0.566, Valid loss: 0.551\n",
      "[Exp  0] Epoch 20\n",
      "Train f1: 0.755, Valid f1: 0.759, Train acc: 0.718, Valid acc: 0.722, Train loss: 0.550, Valid loss: 0.540\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.759, accuracy: 0.722, at epoch 20, loss: 0.540\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 21\n",
      "Train f1: 0.752, Valid f1: 0.743, Train acc: 0.715, Valid acc: 0.649, Train loss: 0.549, Valid loss: 0.693\n",
      "[Exp  0] Epoch 22\n",
      "Train f1: 0.756, Valid f1: 0.737, Train acc: 0.717, Valid acc: 0.728, Train loss: 0.546, Valid loss: 0.546\n",
      "[Exp  0] Epoch 23\n",
      "Train f1: 0.751, Valid f1: 0.573, Train acc: 0.713, Valid acc: 0.657, Train loss: 0.554, Valid loss: 0.618\n",
      "[Exp  0] Epoch 24\n",
      "Train f1: 0.753, Valid f1: 0.769, Train acc: 0.721, Valid acc: 0.732, Train loss: 0.546, Valid loss: 0.534\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.769, accuracy: 0.732, at epoch 24, loss: 0.534\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 25\n",
      "Train f1: 0.760, Valid f1: 0.755, Train acc: 0.727, Valid acc: 0.736, Train loss: 0.542, Valid loss: 0.533\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.755, accuracy: 0.736, at epoch 25, loss: 0.533\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 26\n",
      "Train f1: 0.761, Valid f1: 0.757, Train acc: 0.724, Valid acc: 0.723, Train loss: 0.538, Valid loss: 0.532\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.757, accuracy: 0.723, at epoch 26, loss: 0.532\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 27\n",
      "Train f1: 0.761, Valid f1: 0.732, Train acc: 0.729, Valid acc: 0.726, Train loss: 0.538, Valid loss: 0.529\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.732, accuracy: 0.726, at epoch 27, loss: 0.529\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 28\n",
      "Train f1: 0.773, Valid f1: 0.762, Train acc: 0.736, Valid acc: 0.715, Train loss: 0.531, Valid loss: 0.547\n",
      "[Exp  0] Epoch 29\n",
      "Train f1: 0.770, Valid f1: 0.700, Train acc: 0.736, Valid acc: 0.710, Train loss: 0.530, Valid loss: 0.554\n",
      "[Exp  0] Epoch 30\n",
      "Train f1: 0.762, Valid f1: 0.755, Train acc: 0.727, Valid acc: 0.678, Train loss: 0.532, Valid loss: 0.582\n",
      "[Exp  0] Epoch 31\n",
      "Train f1: 0.770, Valid f1: 0.764, Train acc: 0.735, Valid acc: 0.736, Train loss: 0.524, Valid loss: 0.511\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.764, accuracy: 0.736, at epoch 31, loss: 0.511\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 32\n",
      "Train f1: 0.766, Valid f1: 0.521, Train acc: 0.728, Valid acc: 0.635, Train loss: 0.539, Valid loss: 0.689\n",
      "[Exp  0] Epoch 33\n",
      "Train f1: 0.756, Valid f1: 0.759, Train acc: 0.723, Valid acc: 0.690, Train loss: 0.541, Valid loss: 0.602\n",
      "[Exp  0] Epoch 34\n",
      "Train f1: 0.770, Valid f1: 0.727, Train acc: 0.735, Valid acc: 0.727, Train loss: 0.530, Valid loss: 0.523\n",
      "[Exp  0] Epoch 35\n",
      "Train f1: 0.760, Valid f1: 0.731, Train acc: 0.727, Valid acc: 0.619, Train loss: 0.543, Valid loss: 0.806\n",
      "[Exp  0] Epoch 36\n",
      "Train f1: 0.769, Valid f1: 0.764, Train acc: 0.735, Valid acc: 0.735, Train loss: 0.540, Valid loss: 0.523\n",
      "[Exp  0] Epoch 37\n",
      "Train f1: 0.766, Valid f1: 0.772, Train acc: 0.736, Valid acc: 0.731, Train loss: 0.525, Valid loss: 0.515\n",
      "[Exp  0] Epoch 38\n",
      "Train f1: 0.776, Valid f1: 0.749, Train acc: 0.741, Valid acc: 0.724, Train loss: 0.522, Valid loss: 0.529\n",
      "[Exp  0] Epoch 39\n",
      "Train f1: 0.777, Valid f1: 0.762, Train acc: 0.743, Valid acc: 0.736, Train loss: 0.521, Valid loss: 0.518\n",
      "[Exp  0] Epoch 40\n",
      "Train f1: 0.773, Valid f1: 0.760, Train acc: 0.740, Valid acc: 0.696, Train loss: 0.518, Valid loss: 0.596\n",
      "[Exp  0] Epoch 41\n",
      "Train f1: 0.777, Valid f1: 0.772, Train acc: 0.742, Valid acc: 0.739, Train loss: 0.523, Valid loss: 0.518\n",
      "[Exp  0] Epoch 42\n",
      "Train f1: 0.781, Valid f1: 0.773, Train acc: 0.751, Valid acc: 0.729, Train loss: 0.515, Valid loss: 0.524\n",
      "[Exp  0] Epoch 43\n",
      "Train f1: 0.770, Valid f1: 0.692, Train acc: 0.737, Valid acc: 0.709, Train loss: 0.525, Valid loss: 0.559\n",
      "[Exp  0] Epoch 44\n",
      "Train f1: 0.773, Valid f1: 0.746, Train acc: 0.738, Valid acc: 0.740, Train loss: 0.521, Valid loss: 0.514\n",
      "[Exp  0] Epoch 45\n",
      "Train f1: 0.782, Valid f1: 0.766, Train acc: 0.753, Valid acc: 0.700, Train loss: 0.509, Valid loss: 0.565\n",
      "[Exp  0] Epoch 46\n",
      "Train f1: 0.778, Valid f1: 0.769, Train acc: 0.746, Valid acc: 0.712, Train loss: 0.516, Valid loss: 0.568\n",
      "[Exp  0] Epoch 47\n",
      "Train f1: 0.780, Valid f1: 0.778, Train acc: 0.748, Valid acc: 0.726, Train loss: 0.516, Valid loss: 0.527\n",
      "[Exp  0] Epoch 48\n",
      "Train f1: 0.782, Valid f1: 0.772, Train acc: 0.750, Valid acc: 0.750, Train loss: 0.511, Valid loss: 0.521\n",
      "[Exp  0] Epoch 49\n",
      "Train f1: 0.784, Valid f1: 0.769, Train acc: 0.750, Valid acc: 0.752, Train loss: 0.505, Valid loss: 0.498\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.769, accuracy: 0.752, at epoch 49, loss: 0.498\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 50\n",
      "Train f1: 0.777, Valid f1: 0.693, Train acc: 0.745, Valid acc: 0.720, Train loss: 0.511, Valid loss: 0.565\n",
      "[Exp  0] Epoch 51\n",
      "Train f1: 0.787, Valid f1: 0.788, Train acc: 0.753, Valid acc: 0.747, Train loss: 0.505, Valid loss: 0.523\n",
      "[Exp  0] Epoch 52\n",
      "Train f1: 0.781, Valid f1: 0.775, Train acc: 0.747, Valid acc: 0.741, Train loss: 0.506, Valid loss: 0.522\n",
      "[Exp  0] Epoch 53\n",
      "Train f1: 0.783, Valid f1: 0.739, Train acc: 0.753, Valid acc: 0.743, Train loss: 0.510, Valid loss: 0.525\n",
      "[Exp  0] Epoch 54\n",
      "Train f1: 0.781, Valid f1: 0.777, Train acc: 0.751, Valid acc: 0.747, Train loss: 0.508, Valid loss: 0.511\n",
      "[Exp  0] Epoch 55\n",
      "Train f1: 0.775, Valid f1: 0.780, Train acc: 0.742, Valid acc: 0.721, Train loss: 0.513, Valid loss: 0.539\n",
      "[Exp  0] Epoch 56\n",
      "Train f1: 0.785, Valid f1: 0.773, Train acc: 0.754, Valid acc: 0.750, Train loss: 0.499, Valid loss: 0.493\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.773, accuracy: 0.750, at epoch 56, loss: 0.493\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 57\n",
      "Train f1: 0.781, Valid f1: 0.780, Train acc: 0.751, Valid acc: 0.740, Train loss: 0.503, Valid loss: 0.518\n",
      "[Exp  0] Epoch 58\n",
      "Train f1: 0.789, Valid f1: 0.770, Train acc: 0.757, Valid acc: 0.704, Train loss: 0.501, Valid loss: 0.585\n",
      "[Exp  0] Epoch 59\n",
      "Train f1: 0.785, Valid f1: 0.779, Train acc: 0.753, Valid acc: 0.762, Train loss: 0.505, Valid loss: 0.502\n",
      "[Exp  0] Epoch 60\n",
      "Train f1: 0.789, Valid f1: 0.782, Train acc: 0.758, Valid acc: 0.730, Train loss: 0.486, Valid loss: 0.544\n",
      "[Exp  0] Epoch 61\n",
      "Train f1: 0.792, Valid f1: 0.774, Train acc: 0.762, Valid acc: 0.711, Train loss: 0.490, Valid loss: 0.556\n",
      "[Exp  0] Epoch 62\n",
      "Train f1: 0.783, Valid f1: 0.771, Train acc: 0.752, Valid acc: 0.741, Train loss: 0.506, Valid loss: 0.496\n",
      "[Exp  0] Epoch 63\n",
      "Train f1: 0.782, Valid f1: 0.785, Train acc: 0.751, Valid acc: 0.756, Train loss: 0.499, Valid loss: 0.501\n",
      "[Exp  0] Epoch 64\n",
      "Train f1: 0.783, Valid f1: 0.780, Train acc: 0.753, Valid acc: 0.743, Train loss: 0.511, Valid loss: 0.507\n",
      "[Exp  0] Epoch 65\n",
      "Train f1: 0.789, Valid f1: 0.746, Train acc: 0.756, Valid acc: 0.742, Train loss: 0.493, Valid loss: 0.501\n",
      "[Exp  0] Epoch 66\n",
      "Train f1: 0.787, Valid f1: 0.790, Train acc: 0.756, Valid acc: 0.746, Train loss: 0.489, Valid loss: 0.509\n",
      "[Exp  0] Epoch 67\n",
      "Train f1: 0.792, Valid f1: 0.781, Train acc: 0.762, Valid acc: 0.732, Train loss: 0.488, Valid loss: 0.538\n",
      "[Exp  0] Epoch 68\n",
      "Train f1: 0.786, Valid f1: 0.788, Train acc: 0.753, Valid acc: 0.739, Train loss: 0.499, Valid loss: 0.535\n",
      "[Exp  0] Epoch 69\n",
      "Train f1: 0.794, Valid f1: 0.785, Train acc: 0.764, Valid acc: 0.757, Train loss: 0.486, Valid loss: 0.505\n",
      "[Exp  0] Epoch 70\n",
      "Train f1: 0.796, Valid f1: 0.790, Train acc: 0.766, Valid acc: 0.747, Train loss: 0.482, Valid loss: 0.508\n",
      "[Exp  0] Epoch 71\n",
      "Train f1: 0.786, Valid f1: 0.774, Train acc: 0.753, Valid acc: 0.724, Train loss: 0.495, Valid loss: 0.528\n",
      "[Exp  0] Epoch 72\n",
      "Train f1: 0.798, Valid f1: 0.791, Train acc: 0.769, Valid acc: 0.744, Train loss: 0.480, Valid loss: 0.514\n",
      "[Exp  0] Epoch 73\n",
      "Train f1: 0.794, Valid f1: 0.785, Train acc: 0.762, Valid acc: 0.761, Train loss: 0.483, Valid loss: 0.482\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.785, accuracy: 0.761, at epoch 73, loss: 0.482\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 74\n",
      "Train f1: 0.798, Valid f1: 0.781, Train acc: 0.767, Valid acc: 0.765, Train loss: 0.477, Valid loss: 0.485\n",
      "[Exp  0] Epoch 75\n",
      "Train f1: 0.797, Valid f1: 0.797, Train acc: 0.769, Valid acc: 0.756, Train loss: 0.478, Valid loss: 0.509\n",
      "[Exp  0] Epoch 76\n",
      "Train f1: 0.797, Valid f1: 0.789, Train acc: 0.765, Valid acc: 0.771, Train loss: 0.480, Valid loss: 0.479\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.789, accuracy: 0.771, at epoch 76, loss: 0.479\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 77\n",
      "Train f1: 0.800, Valid f1: 0.776, Train acc: 0.768, Valid acc: 0.766, Train loss: 0.473, Valid loss: 0.485\n",
      "[Exp  0] Epoch 78\n",
      "Train f1: 0.793, Valid f1: 0.767, Train acc: 0.766, Valid acc: 0.751, Train loss: 0.472, Valid loss: 0.488\n",
      "[Exp  0] Epoch 79\n",
      "Train f1: 0.793, Valid f1: 0.789, Train acc: 0.759, Valid acc: 0.751, Train loss: 0.484, Valid loss: 0.496\n",
      "[Exp  0] Epoch 80\n",
      "Train f1: 0.794, Valid f1: 0.765, Train acc: 0.764, Valid acc: 0.759, Train loss: 0.478, Valid loss: 0.495\n",
      "[Exp  0] Epoch 81\n",
      "Train f1: 0.788, Valid f1: 0.777, Train acc: 0.759, Valid acc: 0.758, Train loss: 0.490, Valid loss: 0.482\n",
      "[Exp  0] Epoch 82\n",
      "Train f1: 0.799, Valid f1: 0.782, Train acc: 0.771, Valid acc: 0.765, Train loss: 0.480, Valid loss: 0.480\n",
      "[Exp  0] Epoch 83\n",
      "Train f1: 0.793, Valid f1: 0.791, Train acc: 0.762, Valid acc: 0.768, Train loss: 0.480, Valid loss: 0.484\n",
      "[Exp  0] Epoch 84\n",
      "Train f1: 0.798, Valid f1: 0.797, Train acc: 0.770, Valid acc: 0.764, Train loss: 0.467, Valid loss: 0.479\n",
      "[Exp  0] Epoch 85\n",
      "Train f1: 0.805, Valid f1: 0.793, Train acc: 0.775, Valid acc: 0.752, Train loss: 0.468, Valid loss: 0.515\n",
      "[Exp  0] Epoch 86\n",
      "Train f1: 0.796, Valid f1: 0.791, Train acc: 0.768, Valid acc: 0.744, Train loss: 0.477, Valid loss: 0.505\n",
      "[Exp  0] Epoch 87\n",
      "Train f1: 0.796, Valid f1: 0.801, Train acc: 0.767, Valid acc: 0.764, Train loss: 0.468, Valid loss: 0.486\n",
      "[Exp  0] Epoch 88\n",
      "Train f1: 0.795, Valid f1: 0.760, Train acc: 0.766, Valid acc: 0.755, Train loss: 0.476, Valid loss: 0.493\n",
      "[Exp  0] Epoch 89\n",
      "Train f1: 0.800, Valid f1: 0.783, Train acc: 0.774, Valid acc: 0.759, Train loss: 0.470, Valid loss: 0.489\n",
      "[Exp  0] Epoch 90\n",
      "Train f1: 0.801, Valid f1: 0.789, Train acc: 0.771, Valid acc: 0.754, Train loss: 0.465, Valid loss: 0.486\n",
      "[Exp  0] Epoch 91\n",
      "Train f1: 0.800, Valid f1: 0.782, Train acc: 0.773, Valid acc: 0.722, Train loss: 0.469, Valid loss: 0.586\n",
      "[Exp  0] Epoch 92\n",
      "Train f1: 0.802, Valid f1: 0.795, Train acc: 0.775, Valid acc: 0.760, Train loss: 0.471, Valid loss: 0.502\n",
      "[Exp  0] Epoch 93\n",
      "Train f1: 0.797, Valid f1: 0.748, Train acc: 0.768, Valid acc: 0.751, Train loss: 0.479, Valid loss: 0.499\n",
      "[Exp  0] Epoch 94\n",
      "Train f1: 0.795, Valid f1: 0.791, Train acc: 0.769, Valid acc: 0.747, Train loss: 0.471, Valid loss: 0.526\n",
      "[Exp  0] Epoch 95\n",
      "Train f1: 0.801, Valid f1: 0.789, Train acc: 0.775, Valid acc: 0.764, Train loss: 0.469, Valid loss: 0.483\n",
      "[Exp  0] Epoch 96\n",
      "Train f1: 0.808, Valid f1: 0.759, Train acc: 0.779, Valid acc: 0.754, Train loss: 0.463, Valid loss: 0.493\n",
      "[Exp  0] Epoch 97\n",
      "Train f1: 0.806, Valid f1: 0.786, Train acc: 0.778, Valid acc: 0.763, Train loss: 0.463, Valid loss: 0.490\n",
      "[Exp  0] Epoch 98\n",
      "Train f1: 0.798, Valid f1: 0.793, Train acc: 0.771, Valid acc: 0.774, Train loss: 0.463, Valid loss: 0.471\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.793, accuracy: 0.774, at epoch 98, loss: 0.471\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 99\n",
      "Train f1: 0.806, Valid f1: 0.804, Train acc: 0.778, Valid acc: 0.765, Train loss: 0.457, Valid loss: 0.497\n",
      "[Exp  0] Epoch 100\n",
      "Train f1: 0.805, Valid f1: 0.788, Train acc: 0.778, Valid acc: 0.763, Train loss: 0.461, Valid loss: 0.474\n",
      "[Exp  0] Epoch 101\n",
      "Train f1: 0.804, Valid f1: 0.795, Train acc: 0.777, Valid acc: 0.773, Train loss: 0.460, Valid loss: 0.472\n",
      "[Exp  0] Epoch 102\n",
      "Train f1: 0.807, Valid f1: 0.793, Train acc: 0.782, Valid acc: 0.763, Train loss: 0.451, Valid loss: 0.490\n",
      "[Exp  0] Epoch 103\n",
      "Train f1: 0.816, Valid f1: 0.797, Train acc: 0.791, Valid acc: 0.775, Train loss: 0.446, Valid loss: 0.473\n",
      "[Exp  0] Epoch 104\n",
      "Train f1: 0.797, Valid f1: 0.795, Train acc: 0.768, Valid acc: 0.758, Train loss: 0.477, Valid loss: 0.483\n",
      "[Exp  0] Epoch 105\n",
      "Train f1: 0.811, Valid f1: 0.797, Train acc: 0.782, Valid acc: 0.765, Train loss: 0.456, Valid loss: 0.485\n",
      "[Exp  0] Epoch 106\n",
      "Train f1: 0.803, Valid f1: 0.781, Train acc: 0.777, Valid acc: 0.774, Train loss: 0.459, Valid loss: 0.473\n",
      "[Exp  0] Epoch 107\n",
      "Train f1: 0.810, Valid f1: 0.786, Train acc: 0.785, Valid acc: 0.766, Train loss: 0.448, Valid loss: 0.492\n",
      "[Exp  0] Epoch 108\n",
      "Train f1: 0.808, Valid f1: 0.785, Train acc: 0.782, Valid acc: 0.780, Train loss: 0.453, Valid loss: 0.478\n",
      "[Exp  0] Epoch 109\n",
      "Train f1: 0.811, Valid f1: 0.792, Train acc: 0.785, Valid acc: 0.756, Train loss: 0.448, Valid loss: 0.495\n",
      "[Exp  0] Epoch 110\n",
      "Train f1: 0.803, Valid f1: 0.790, Train acc: 0.776, Valid acc: 0.768, Train loss: 0.460, Valid loss: 0.489\n",
      "[Exp  0] Epoch 111\n",
      "Train f1: 0.806, Valid f1: 0.763, Train acc: 0.781, Valid acc: 0.757, Train loss: 0.463, Valid loss: 0.512\n",
      "[Exp  0] Epoch 112\n",
      "Train f1: 0.805, Valid f1: 0.791, Train acc: 0.779, Valid acc: 0.747, Train loss: 0.449, Valid loss: 0.511\n",
      "[Exp  0] Epoch 113\n",
      "Train f1: 0.811, Valid f1: 0.800, Train acc: 0.786, Valid acc: 0.777, Train loss: 0.451, Valid loss: 0.469\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.800, accuracy: 0.777, at epoch 113, loss: 0.469\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 114\n",
      "Train f1: 0.814, Valid f1: 0.789, Train acc: 0.789, Valid acc: 0.780, Train loss: 0.448, Valid loss: 0.472\n",
      "[Exp  0] Epoch 115\n",
      "Train f1: 0.820, Valid f1: 0.765, Train acc: 0.795, Valid acc: 0.759, Train loss: 0.443, Valid loss: 0.479\n",
      "[Exp  0] Epoch 116\n",
      "Train f1: 0.812, Valid f1: 0.782, Train acc: 0.788, Valid acc: 0.769, Train loss: 0.448, Valid loss: 0.478\n",
      "[Exp  0] Epoch 117\n",
      "Train f1: 0.814, Valid f1: 0.799, Train acc: 0.789, Valid acc: 0.755, Train loss: 0.450, Valid loss: 0.517\n",
      "[Exp  0] Epoch 118\n",
      "Train f1: 0.813, Valid f1: 0.768, Train acc: 0.787, Valid acc: 0.769, Train loss: 0.449, Valid loss: 0.499\n",
      "[Exp  0] Epoch 119\n",
      "Train f1: 0.815, Valid f1: 0.796, Train acc: 0.791, Valid acc: 0.783, Train loss: 0.436, Valid loss: 0.475\n",
      "[Exp  0] Epoch 120\n",
      "Train f1: 0.815, Valid f1: 0.800, Train acc: 0.789, Valid acc: 0.780, Train loss: 0.438, Valid loss: 0.479\n",
      "[Exp  0] Epoch 121\n",
      "Train f1: 0.820, Valid f1: 0.797, Train acc: 0.796, Valid acc: 0.759, Train loss: 0.436, Valid loss: 0.491\n",
      "[Exp  0] Epoch 122\n",
      "Train f1: 0.818, Valid f1: 0.775, Train acc: 0.794, Valid acc: 0.773, Train loss: 0.435, Valid loss: 0.471\n",
      "[Exp  0] Epoch 123\n",
      "Train f1: 0.814, Valid f1: 0.801, Train acc: 0.789, Valid acc: 0.765, Train loss: 0.447, Valid loss: 0.491\n",
      "[Exp  0] Epoch 124\n",
      "Train f1: 0.813, Valid f1: 0.783, Train acc: 0.789, Valid acc: 0.771, Train loss: 0.443, Valid loss: 0.480\n",
      "[Exp  0] Epoch 125\n",
      "Train f1: 0.812, Valid f1: 0.771, Train acc: 0.788, Valid acc: 0.757, Train loss: 0.444, Valid loss: 0.491\n",
      "[Exp  0] Epoch 126\n",
      "Train f1: 0.815, Valid f1: 0.809, Train acc: 0.791, Valid acc: 0.774, Train loss: 0.444, Valid loss: 0.483\n",
      "[Exp  0] Epoch 127\n",
      "Train f1: 0.822, Valid f1: 0.770, Train acc: 0.798, Valid acc: 0.773, Train loss: 0.432, Valid loss: 0.483\n",
      "[Exp  0] Epoch 128\n",
      "Train f1: 0.818, Valid f1: 0.802, Train acc: 0.794, Valid acc: 0.771, Train loss: 0.434, Valid loss: 0.477\n",
      "[Exp  0] Epoch 129\n",
      "Train f1: 0.824, Valid f1: 0.797, Train acc: 0.801, Valid acc: 0.763, Train loss: 0.428, Valid loss: 0.515\n",
      "[Exp  0] Epoch 130\n",
      "Train f1: 0.811, Valid f1: 0.799, Train acc: 0.788, Valid acc: 0.757, Train loss: 0.432, Valid loss: 0.517\n",
      "[Exp  0] Epoch 131\n",
      "Train f1: 0.817, Valid f1: 0.806, Train acc: 0.793, Valid acc: 0.781, Train loss: 0.429, Valid loss: 0.480\n",
      "[Exp  0] Epoch 132\n",
      "Train f1: 0.820, Valid f1: 0.789, Train acc: 0.794, Valid acc: 0.779, Train loss: 0.435, Valid loss: 0.464\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.789, accuracy: 0.779, at epoch 132, loss: 0.464\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 133\n",
      "Train f1: 0.823, Valid f1: 0.805, Train acc: 0.800, Valid acc: 0.774, Train loss: 0.428, Valid loss: 0.476\n",
      "[Exp  0] Epoch 134\n",
      "Train f1: 0.819, Valid f1: 0.806, Train acc: 0.796, Valid acc: 0.781, Train loss: 0.435, Valid loss: 0.497\n",
      "[Exp  0] Epoch 135\n",
      "Train f1: 0.824, Valid f1: 0.799, Train acc: 0.802, Valid acc: 0.768, Train loss: 0.424, Valid loss: 0.476\n",
      "[Exp  0] Epoch 136\n",
      "Train f1: 0.821, Valid f1: 0.771, Train acc: 0.796, Valid acc: 0.766, Train loss: 0.427, Valid loss: 0.483\n",
      "[Exp  0] Epoch 137\n",
      "Train f1: 0.818, Valid f1: 0.800, Train acc: 0.795, Valid acc: 0.781, Train loss: 0.432, Valid loss: 0.464\n",
      "[Exp  0] Epoch 138\n",
      "Train f1: 0.820, Valid f1: 0.782, Train acc: 0.799, Valid acc: 0.772, Train loss: 0.424, Valid loss: 0.478\n",
      "[Exp  0] Epoch 139\n",
      "Train f1: 0.820, Valid f1: 0.790, Train acc: 0.794, Valid acc: 0.771, Train loss: 0.430, Valid loss: 0.470\n",
      "[Exp  0] Epoch 140\n",
      "Train f1: 0.813, Valid f1: 0.799, Train acc: 0.790, Valid acc: 0.787, Train loss: 0.439, Valid loss: 0.456\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.799, accuracy: 0.787, at epoch 140, loss: 0.456\n",
      "------------------------------- \n",
      "[Exp  0] Epoch 141\n",
      "Train f1: 0.819, Valid f1: 0.798, Train acc: 0.796, Valid acc: 0.784, Train loss: 0.435, Valid loss: 0.465\n",
      "[Exp  0] Epoch 142\n",
      "Train f1: 0.821, Valid f1: 0.773, Train acc: 0.799, Valid acc: 0.770, Train loss: 0.422, Valid loss: 0.487\n",
      "[Exp  0] Epoch 143\n",
      "Train f1: 0.818, Valid f1: 0.797, Train acc: 0.792, Valid acc: 0.772, Train loss: 0.428, Valid loss: 0.477\n",
      "[Exp  0] Epoch 144\n",
      "Train f1: 0.819, Valid f1: 0.797, Train acc: 0.796, Valid acc: 0.769, Train loss: 0.424, Valid loss: 0.487\n",
      "[Exp  0] Epoch 145\n",
      "Train f1: 0.820, Valid f1: 0.806, Train acc: 0.796, Valid acc: 0.775, Train loss: 0.426, Valid loss: 0.491\n",
      "[Exp  0] Epoch 146\n",
      "Train f1: 0.819, Valid f1: 0.778, Train acc: 0.796, Valid acc: 0.775, Train loss: 0.423, Valid loss: 0.494\n",
      "[Exp  0] Epoch 147\n",
      "Train f1: 0.817, Valid f1: 0.797, Train acc: 0.795, Valid acc: 0.773, Train loss: 0.433, Valid loss: 0.470\n",
      "[Exp  0] Epoch 148\n",
      "Train f1: 0.824, Valid f1: 0.802, Train acc: 0.803, Valid acc: 0.786, Train loss: 0.417, Valid loss: 0.466\n",
      "[Exp  0] Epoch 149\n",
      "Train f1: 0.825, Valid f1: 0.796, Train acc: 0.803, Valid acc: 0.774, Train loss: 0.415, Valid loss: 0.471\n",
      "[Exp  0] Epoch 150\n",
      "Train f1: 0.820, Valid f1: 0.788, Train acc: 0.797, Valid acc: 0.781, Train loss: 0.420, Valid loss: 0.478\n",
      "[Exp  0] Epoch 151\n",
      "Train f1: 0.829, Valid f1: 0.795, Train acc: 0.807, Valid acc: 0.763, Train loss: 0.421, Valid loss: 0.489\n",
      "[Exp  0] Epoch 152\n",
      "Train f1: 0.823, Valid f1: 0.799, Train acc: 0.800, Valid acc: 0.784, Train loss: 0.419, Valid loss: 0.468\n",
      "[Exp  0] Epoch 153\n",
      "Train f1: 0.823, Valid f1: 0.776, Train acc: 0.800, Valid acc: 0.765, Train loss: 0.415, Valid loss: 0.489\n",
      "[Exp  0] Epoch 154\n",
      "Train f1: 0.817, Valid f1: 0.792, Train acc: 0.795, Valid acc: 0.784, Train loss: 0.421, Valid loss: 0.468\n",
      "[Exp  0] Epoch 155\n",
      "Train f1: 0.823, Valid f1: 0.783, Train acc: 0.802, Valid acc: 0.768, Train loss: 0.420, Valid loss: 0.486\n",
      "[Exp  0] Epoch 156\n",
      "Train f1: 0.832, Valid f1: 0.794, Train acc: 0.810, Valid acc: 0.769, Train loss: 0.405, Valid loss: 0.482\n",
      "[Exp  0] Epoch 157\n",
      "Train f1: 0.824, Valid f1: 0.776, Train acc: 0.802, Valid acc: 0.769, Train loss: 0.424, Valid loss: 0.483\n",
      "[Exp  0] Epoch 158\n",
      "Train f1: 0.824, Valid f1: 0.796, Train acc: 0.802, Valid acc: 0.777, Train loss: 0.415, Valid loss: 0.477\n",
      "[Exp  0] Epoch 159\n",
      "Train f1: 0.831, Valid f1: 0.808, Train acc: 0.810, Valid acc: 0.782, Train loss: 0.407, Valid loss: 0.473\n",
      "[Exp  0] Epoch 160\n",
      "Train f1: 0.826, Valid f1: 0.801, Train acc: 0.807, Valid acc: 0.786, Train loss: 0.411, Valid loss: 0.463\n",
      "[Exp  0] Epoch 161\n",
      "Train f1: 0.826, Valid f1: 0.801, Train acc: 0.806, Valid acc: 0.787, Train loss: 0.412, Valid loss: 0.475\n",
      "[Exp  0] Epoch 162\n",
      "Train f1: 0.832, Valid f1: 0.764, Train acc: 0.810, Valid acc: 0.763, Train loss: 0.409, Valid loss: 0.510\n",
      "[Exp  0] Epoch 163\n",
      "Train f1: 0.824, Valid f1: 0.798, Train acc: 0.803, Valid acc: 0.786, Train loss: 0.422, Valid loss: 0.467\n",
      "[Exp  0] Epoch 164\n",
      "Train f1: 0.826, Valid f1: 0.800, Train acc: 0.806, Valid acc: 0.754, Train loss: 0.406, Valid loss: 0.574\n",
      "[Exp  0] Epoch 165\n",
      "Train f1: 0.828, Valid f1: 0.801, Train acc: 0.805, Valid acc: 0.781, Train loss: 0.411, Valid loss: 0.469\n",
      "[Exp  0] Epoch 166\n",
      "Train f1: 0.822, Valid f1: 0.805, Train acc: 0.803, Valid acc: 0.776, Train loss: 0.409, Valid loss: 0.479\n",
      "[Exp  0] Epoch 167\n",
      "Train f1: 0.829, Valid f1: 0.757, Train acc: 0.808, Valid acc: 0.760, Train loss: 0.407, Valid loss: 0.499\n",
      "[Exp  0] Epoch 168\n",
      "Train f1: 0.831, Valid f1: 0.802, Train acc: 0.811, Valid acc: 0.777, Train loss: 0.400, Valid loss: 0.478\n",
      "[Exp  0] Epoch 169\n",
      "Train f1: 0.830, Valid f1: 0.800, Train acc: 0.810, Valid acc: 0.780, Train loss: 0.396, Valid loss: 0.493\n",
      "[Exp  0] Epoch 170\n",
      "Train f1: 0.827, Valid f1: 0.794, Train acc: 0.805, Valid acc: 0.786, Train loss: 0.409, Valid loss: 0.483\n",
      "[Exp  0] Epoch 171\n",
      "Train f1: 0.827, Valid f1: 0.789, Train acc: 0.806, Valid acc: 0.771, Train loss: 0.408, Valid loss: 0.487\n",
      "[Exp  0] Epoch 172\n",
      "Train f1: 0.831, Valid f1: 0.805, Train acc: 0.812, Valid acc: 0.786, Train loss: 0.401, Valid loss: 0.466\n",
      "[Exp  0] Epoch 173\n",
      "Train f1: 0.826, Valid f1: 0.797, Train acc: 0.802, Valid acc: 0.772, Train loss: 0.404, Valid loss: 0.468\n",
      "[Exp  0] Epoch 174\n",
      "Train f1: 0.833, Valid f1: 0.799, Train acc: 0.812, Valid acc: 0.780, Train loss: 0.401, Valid loss: 0.469\n",
      "[Exp  0] Epoch 175\n",
      "Train f1: 0.821, Valid f1: 0.809, Train acc: 0.802, Valid acc: 0.781, Train loss: 0.422, Valid loss: 0.485\n",
      "[Exp  0] Epoch 176\n",
      "Train f1: 0.831, Valid f1: 0.803, Train acc: 0.809, Valid acc: 0.778, Train loss: 0.399, Valid loss: 0.504\n",
      "[Exp  0] Epoch 177\n",
      "Train f1: 0.831, Valid f1: 0.791, Train acc: 0.810, Valid acc: 0.773, Train loss: 0.394, Valid loss: 0.504\n",
      "[Exp  0] Epoch 178\n",
      "Train f1: 0.836, Valid f1: 0.805, Train acc: 0.816, Valid acc: 0.766, Train loss: 0.401, Valid loss: 0.497\n",
      "[Exp  0] Epoch 179\n",
      "Train f1: 0.831, Valid f1: 0.792, Train acc: 0.811, Valid acc: 0.772, Train loss: 0.398, Valid loss: 0.476\n",
      "[Exp  0] Epoch 180\n",
      "Train f1: 0.833, Valid f1: 0.798, Train acc: 0.813, Valid acc: 0.777, Train loss: 0.398, Valid loss: 0.469\n",
      "[Exp  0] Epoch 181\n",
      "Train f1: 0.842, Valid f1: 0.813, Train acc: 0.823, Valid acc: 0.793, Train loss: 0.395, Valid loss: 0.465\n",
      "[Exp  0] Epoch 182\n",
      "Train f1: 0.831, Valid f1: 0.798, Train acc: 0.814, Valid acc: 0.774, Train loss: 0.392, Valid loss: 0.486\n",
      "[Exp  0] Epoch 183\n",
      "Train f1: 0.833, Valid f1: 0.787, Train acc: 0.814, Valid acc: 0.784, Train loss: 0.392, Valid loss: 0.487\n",
      "[Exp  0] Epoch 184\n",
      "Train f1: 0.831, Valid f1: 0.801, Train acc: 0.813, Valid acc: 0.789, Train loss: 0.401, Valid loss: 0.468\n",
      "[Exp  0] Epoch 185\n",
      "Train f1: 0.827, Valid f1: 0.786, Train acc: 0.808, Valid acc: 0.777, Train loss: 0.406, Valid loss: 0.505\n",
      "[Exp  0] Epoch 186\n",
      "Train f1: 0.831, Valid f1: 0.799, Train acc: 0.811, Valid acc: 0.784, Train loss: 0.407, Valid loss: 0.475\n",
      "[Exp  0] Epoch 187\n",
      "Train f1: 0.840, Valid f1: 0.800, Train acc: 0.820, Valid acc: 0.759, Train loss: 0.389, Valid loss: 0.513\n",
      "[Exp  0] Epoch 188\n",
      "Train f1: 0.832, Valid f1: 0.802, Train acc: 0.810, Valid acc: 0.783, Train loss: 0.397, Valid loss: 0.471\n",
      "[Exp  0] Epoch 189\n",
      "Train f1: 0.832, Valid f1: 0.778, Train acc: 0.810, Valid acc: 0.774, Train loss: 0.392, Valid loss: 0.499\n",
      "[Exp  0] Epoch 190\n",
      "Train f1: 0.829, Valid f1: 0.810, Train acc: 0.809, Valid acc: 0.790, Train loss: 0.405, Valid loss: 0.484\n",
      "[Exp  0] Epoch 191\n",
      "Train f1: 0.840, Valid f1: 0.789, Train acc: 0.823, Valid acc: 0.753, Train loss: 0.388, Valid loss: 0.539\n",
      "[Exp  0] Epoch 192\n",
      "Train f1: 0.825, Valid f1: 0.805, Train acc: 0.806, Valid acc: 0.792, Train loss: 0.417, Valid loss: 0.468\n",
      "[Exp  0] Epoch 193\n",
      "Train f1: 0.835, Valid f1: 0.806, Train acc: 0.815, Valid acc: 0.783, Train loss: 0.395, Valid loss: 0.473\n",
      "[Exp  0] Epoch 194\n",
      "Train f1: 0.830, Valid f1: 0.801, Train acc: 0.810, Valid acc: 0.781, Train loss: 0.398, Valid loss: 0.472\n",
      "[Exp  0] Epoch 195\n",
      "Train f1: 0.836, Valid f1: 0.799, Train acc: 0.816, Valid acc: 0.762, Train loss: 0.390, Valid loss: 0.553\n",
      "[Exp  0] Epoch 196\n",
      "Train f1: 0.836, Valid f1: 0.812, Train acc: 0.818, Valid acc: 0.784, Train loss: 0.387, Valid loss: 0.508\n",
      "[Exp  0] Epoch 197\n",
      "Train f1: 0.835, Valid f1: 0.811, Train acc: 0.815, Valid acc: 0.798, Train loss: 0.392, Valid loss: 0.475\n",
      "[Exp  0] Epoch 198\n",
      "Train f1: 0.836, Valid f1: 0.800, Train acc: 0.816, Valid acc: 0.786, Train loss: 0.388, Valid loss: 0.466\n",
      "[Exp  0] Epoch 199\n",
      "Train f1: 0.837, Valid f1: 0.807, Train acc: 0.816, Valid acc: 0.781, Train loss: 0.387, Valid loss: 0.499\n",
      "[Exp  0] Epoch 200\n",
      "Train f1: 0.845, Valid f1: 0.803, Train acc: 0.829, Valid acc: 0.777, Train loss: 0.369, Valid loss: 0.517\n",
      "[Exp  0] Epoch 201\n",
      "Train f1: 0.837, Valid f1: 0.763, Train acc: 0.819, Valid acc: 0.768, Train loss: 0.386, Valid loss: 0.516\n",
      "[Exp  0] Epoch 202\n",
      "Train f1: 0.834, Valid f1: 0.792, Train acc: 0.817, Valid acc: 0.786, Train loss: 0.392, Valid loss: 0.468\n",
      "[Exp  0] Epoch 203\n",
      "Train f1: 0.842, Valid f1: 0.791, Train acc: 0.825, Valid acc: 0.765, Train loss: 0.372, Valid loss: 0.514\n",
      "[Exp  0] Epoch 204\n",
      "Train f1: 0.844, Valid f1: 0.803, Train acc: 0.827, Valid acc: 0.777, Train loss: 0.375, Valid loss: 0.515\n",
      "[Exp  0] Epoch 205\n",
      "Train f1: 0.843, Valid f1: 0.766, Train acc: 0.824, Valid acc: 0.767, Train loss: 0.385, Valid loss: 0.533\n",
      "[Exp  0] Epoch 206\n",
      "Train f1: 0.834, Valid f1: 0.792, Train acc: 0.815, Valid acc: 0.777, Train loss: 0.396, Valid loss: 0.461\n",
      "[Exp  0] Epoch 207\n",
      "Train f1: 0.834, Valid f1: 0.780, Train acc: 0.817, Valid acc: 0.780, Train loss: 0.382, Valid loss: 0.489\n",
      "[Exp  0] Epoch 208\n",
      "Train f1: 0.840, Valid f1: 0.763, Train acc: 0.821, Valid acc: 0.765, Train loss: 0.382, Valid loss: 0.519\n",
      "[Exp  0] Epoch 209\n",
      "Train f1: 0.834, Valid f1: 0.789, Train acc: 0.817, Valid acc: 0.782, Train loss: 0.388, Valid loss: 0.471\n",
      "[Exp  0] Epoch 210\n",
      "Train f1: 0.841, Valid f1: 0.792, Train acc: 0.824, Valid acc: 0.778, Train loss: 0.381, Valid loss: 0.488\n",
      "[Exp  0] Epoch 211\n",
      "Train f1: 0.842, Valid f1: 0.804, Train acc: 0.825, Valid acc: 0.787, Train loss: 0.374, Valid loss: 0.494\n",
      "[Exp  0] Epoch 212\n",
      "Train f1: 0.836, Valid f1: 0.806, Train acc: 0.820, Valid acc: 0.786, Train loss: 0.383, Valid loss: 0.479\n",
      "[Exp  0] Epoch 213\n",
      "Train f1: 0.835, Valid f1: 0.791, Train acc: 0.816, Valid acc: 0.753, Train loss: 0.395, Valid loss: 0.515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d6e91c39918a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-337c7c2ca19d>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(cnt_exp, partition, args)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;31m#         val_loss = validate(model, partition['val'], criterion, args)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-337c7c2ca19d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlist_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlist_y\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kyle/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kyle/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time \n",
    "from sklearn.metrics import mean_absolute_error, f1_score, accuracy_score, log_loss\n",
    "from utils import *\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# ==== Embedding Config ==== #\n",
    "args.max_len = MAX_LEN\n",
    "args.vocab_size = 40\n",
    "args.degree_size = 6\n",
    "args.numH_size = 5\n",
    "args.valence_size = 6\n",
    "args.isarom_size = 2\n",
    "args.emb_train = True\n",
    "\n",
    "\n",
    "# ==== Model Architecture Config ==== #\n",
    "args.in_dim = 64\n",
    "args.out_dim = 256\n",
    "args.molvec_dim = 512\n",
    "args.n_layer = 1\n",
    "args.use_bn = True\n",
    "args.act = 'relu'\n",
    "args.dp_rate = 0.5\n",
    "\n",
    "\n",
    "# ==== Optimizer Config ==== #\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "\n",
    "\n",
    "# ==== Training Config ==== #\n",
    "args.epoch = 1000\n",
    "args.batch_size = 256\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.exp_name = 'exp1_lr_stage'\n",
    "\n",
    "\n",
    "writer = Writer(prior_keyword=['n_layer', 'use_bn', 'lr', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
    "writer.clear()\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "#list_n_layer = [1]\n",
    "list_lr = [0.001, 0.005]\n",
    "list_n_layer = [3,4,5]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(gcnDataset(df_train, args.max_len), batch_size=args.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(gcnDataset(df_valid, args.max_len), batch_size=args.batch_size, shuffle=False)\n",
    "# test_dataloader = DataLoader(gcnDataset(datasets[2], args.max_len), batch_size=args.batch_size, shuffle=False)\n",
    "partition = {'train': train_dataloader, 'val': val_dataloader }#, 'test': test_dataloader}\n",
    "\n",
    "cnt_exp = 0\n",
    "for lr in list_lr:\n",
    "    for n_layer in list_n_layer:\n",
    "        args.lr = lr\n",
    "        args.n_layer = n_layer\n",
    "\n",
    "        model, result = experiment(cnt_exp, partition, args)\n",
    "        writer.write(result)\n",
    "        \n",
    "        cnt_exp += 1\n",
    "        print('[Exp {:2}] got f1_score: {:2.3f}, accuracy: {:2.3f}, std: {:2.3f}, valid_loss{:2.3f} at epoch {:2} took {:3.1f} sec'.format(cnt_exp, result.best_f1_score, result.best_accuracy, result.best_std, result.best_valid_loss, result.best_epoch, result.elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "results = writer.read(exp_name='exp1_lr_stage')\n",
    "variable1 = 'n_layer'\n",
    "variable2 = 'lr'\n",
    "\n",
    "\n",
    "plot_performance(results, variable1, variable2, args,\n",
    "                'Performance depends on {} vs {}'.format(variable1, variable2),\n",
    "                'exp1_Performance {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_distribution(results, variable1, variable2, 'true_y', 'pred_y', \n",
    "                  'Prediction results depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Prediction {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_loss(results, variable1, variable2, 'epoch', 'loss', \n",
    "                  'Loss depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Loss {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualizing Embedding Matrix  \n",
    "\n",
    "`args.emb_train=True`로 하게 되면 one-hot-encoding된 임베딩 weight의 값들도 Loss를 최소화하는 방향으로 학습될 수 있습니다. 실제로 학습된 모델의 embedding matrix를 시각화하여 값이 변화했는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.imshow(model.embedding[0].weight.detach().cpu().numpy(), interpolation='nearest', cmap=plt.cm.Reds)\n",
    "ax.set_yticks(np.arange(len(LIST_SYMBOLS)+1))\n",
    "ax.set_xticks(np.arange(len(LIST_SYMBOLS)+1))\n",
    "\n",
    "ax.set_yticklabels(['PAD'] + LIST_SYMBOLS)\n",
    "ax.set_title(\"Embedding Weight Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

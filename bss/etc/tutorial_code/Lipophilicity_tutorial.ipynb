{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to \n",
    "- https://github.com/heartcored98/Standalone-DeepLearning-Chemistry/blob/master/Lec05/Lec05_lipo_graph_gcn_prediction.ipynb\n",
    "- https://untitledtblog.tistory.com/152\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [00:18:32] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_split_data(train_ratio=0.8, valid_ratio=0.1, test_ratio=0.1,\n",
    "                   seed=2020):\n",
    "\n",
    "    # load dataset\n",
    "    df = pd.read_csv('../reference_data/Lipophilicity.csv')\n",
    "\n",
    "    train_val, test = train_test_split(df,\n",
    "                                       test_size=test_ratio,\n",
    "                                       random_state=seed)\n",
    "    \n",
    "    train, val = train_test_split(train_val,\n",
    "                                  test_size=valid_ratio/(train_ratio+valid_ratio),\n",
    "                                  random_state=seed)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../reference_data/Lipophilicity.csv')\n",
    "df_tot = get_split_data()\n",
    "df_train = df_tot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CMPD_CHEMBLID</th>\n",
       "      <th>exp</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>CHEMBL217620</td>\n",
       "      <td>4.20</td>\n",
       "      <td>CC(C)N(C(C)C)C(=O)C(C(CNC(=O)NCc1ccccc1F)c2ccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>CHEMBL1738761</td>\n",
       "      <td>0.76</td>\n",
       "      <td>NCC[C@@H](Oc1cc(Cl)ccc1C#N)c2ccccc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3236</td>\n",
       "      <td>CHEMBL249295</td>\n",
       "      <td>3.20</td>\n",
       "      <td>CN1CCN(CC1)[C@@H]2C[C@@H](C2)c3nc(c4ccc5ccc(nc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>CHEMBL1319232</td>\n",
       "      <td>2.10</td>\n",
       "      <td>Cc1ccc(cc1)N2NC(=O)c3cccnc23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>CHEMBL1481866</td>\n",
       "      <td>1.70</td>\n",
       "      <td>NC(=O)c1ccc(Oc2cccc3cccnc23)c(c1)[N+](=O)[O-]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CMPD_CHEMBLID   exp                                             smiles\n",
       "1809   CHEMBL217620  4.20  CC(C)N(C(C)C)C(=O)C(C(CNC(=O)NCc1ccccc1F)c2ccc...\n",
       "397   CHEMBL1738761  0.76                NCC[C@@H](Oc1cc(Cl)ccc1C#N)c2ccccc2\n",
       "3236   CHEMBL249295  3.20  CN1CCN(CC1)[C@@H]2C[C@@H](C2)c3nc(c4ccc5ccc(nc...\n",
       "2330  CHEMBL1319232  2.10                       Cc1ccc(cc1)N2NC(=O)c3cccnc23\n",
       "152   CHEMBL1481866  1.70      NC(=O)c1ccc(Oc2cccc3cccnc23)c(c1)[N+](=O)[O-]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 construct Mol2Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_atom_symbols(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    symbols = set([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return symbols\n",
    "\n",
    "def get_num_atom(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    num_atom = len([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return num_atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the whole symbol set (and apply unique)\n",
    "# extract the whole num of atoms (and apply max)\n",
    "\n",
    "LIST_SYMBOLS = list(set.union(*df['smiles'].apply(lambda x: get_unique_atom_symbols(x)).values))\n",
    "MAX_LEN = max(df['smiles'].apply(lambda x: get_num_atom(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Se', 'B', 'S', 'N', 'I', 'Br', 'O', 'C', 'F', 'Si', 'Cl', 'P'], 115, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIST_SYMBOLS, MAX_LEN, len(LIST_SYMBOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_feature(atom):\n",
    "    # 1) atom symbol\n",
    "    # 2) degree\n",
    "    # 3) 붙어 있는 수소 원자의 수\n",
    "    # 4) Valence (원자가는 어떤 원자가 다른 원자들과 어느 정도 수준으로 공유 결합을 이루는가를 나타내는 척도)\n",
    "    # 5) Aromatic 여부 (평평한 고리 구조를 가진 방향족인지)\n",
    "    return np.array(char_to_ix(atom.GetSymbol(), LIST_SYMBOLS) +\n",
    "                    char_to_ix(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    char_to_ix(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(int(atom.GetIsAromatic()), [0, 1]))    # (40, 6, 5, 6, 2)\n",
    "\n",
    "def char_to_ix(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        return [0] # Unknown Atom Token\n",
    "    return [allowable_set.index(x)+1]\n",
    "\n",
    "def mol2graph(smi, MAX_LEN):\n",
    "    # MAX_LEN 최대 원자 수\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "    X = np.zeros((MAX_LEN, 5), dtype=np.uint8)\n",
    "    A = np.zeros((MAX_LEN, MAX_LEN), dtype=np.uint8)\n",
    "\n",
    "    temp_A = Chem.rdmolops.GetAdjacencyMatrix(mol\n",
    "                                             ).astype(np.uint8, copy=False)[:MAX_LEN, :MAX_LEN]\n",
    "    num_atom = temp_A.shape[0]\n",
    "    eye_matrix = np.eye(temp_A.shape[0], dtype=np.uint8)\n",
    "    A[:num_atom, :num_atom] = temp_A + eye_matrix\n",
    "    \n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        feature = atom_feature(atom)\n",
    "        X[i, :] = feature\n",
    "        if i + 1 >= num_atom: break\n",
    "            \n",
    "    return X, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Construct Dataset/DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class gcnDataset(Dataset):\n",
    "    def __init__(self, df, max_len=MAX_LEN):\n",
    "        self.smiles = df[\"smiles\"]\n",
    "        self.exp = df[\"exp\"].values\n",
    "                \n",
    "        list_X, list_A = [], []\n",
    "        for i, smiles in enumerate(self.smiles):\n",
    "            X, A = mol2graph(smiles, max_len)\n",
    "            list_X.append(X)\n",
    "            list_A.append(A)\n",
    "            \n",
    "        self.X = np.array(list_X, dtype=np.uint8)\n",
    "        self.A = np.array(list_A, dtype=np.uint8)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.A[index], self.exp[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "             \n",
    "    def forward(self, x):\n",
    "\n",
    "        # When skip BN\n",
    "        if not self.use_bn:\n",
    "            output = x\n",
    "            return  output\n",
    "        \n",
    "        # When use BN\n",
    "        output = x.view(-1, x.shape[-1])\n",
    "        output = self.bn(output)\n",
    "        output = output.view(x.shape)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = BN1d(output_dim, use_bn)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        # invariant node feature을 transformation\n",
    "        x = self.fc(X) #X'=WX+b (W=(output_dim,input_dim), X=(output_dim,feature_dim))\n",
    "        x = torch.matmul(A, x) # A*X'\n",
    "        x = self.relu(self.bn(x)) # relu(A*X')\n",
    "        return x, A\n",
    "          \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
    "\n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = torch.mean(molvec, dim=1)\n",
    "        return molvec\n",
    "    \n",
    "\n",
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        # Create Atom Element embedding layer\n",
    "        self.embedding = self.create_emb_layer([args.vocab_size, args.degree_size,\n",
    "                                                args.numH_size, args.valence_size,\n",
    "                                                args.isarom_size],  args.emb_train)    \n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        for i in range(args.n_layer):\n",
    "            self.gcn_layers.append(GConv(args.in_dim if i==0 else args.out_dim, args.out_dim, args.use_bn))\n",
    "                                   \n",
    "        self.readout = Readout(args.out_dim, args.molvec_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.molvec_dim, args.molvec_dim//2)\n",
    "        self.fc2 = nn.Linear(args.molvec_dim//2, args.molvec_dim//2)\n",
    "        self.fc3 = nn.Linear(args.molvec_dim//2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def create_emb_layer(self, list_vocab_size, emb_train=False):\n",
    "        '''\n",
    "            list_vocab_size (embed_size for 5 features)\n",
    "            - vocab_size = 12 (총 원소개수 12개임)\n",
    "            - degree_size = 6 (0,1,2,3,4,5)\n",
    "            - numH_size = 5 (0,1,2,3,4)\n",
    "            - valence_size = 6 (0,1,2,3,4,5)\n",
    "            - isarom_size = 2 (y/n)\n",
    "        '''\n",
    "        list_emb_layer = nn.ModuleList()\n",
    "        for i, vocab_size in enumerate(list_vocab_size):\n",
    "            \n",
    "            # \n",
    "            vocab_size += 1\n",
    "            \n",
    "            # nn.Embedding\n",
    "            ## num_embeddings : 임베딩을 할 단어들의 개수. 다시 말해 단어 집합의 크기입니다.\n",
    "            ## embedding_dim : 임베딩 할 벡터의 차원입니다. 사용자가 정해주는 하이퍼파라미터입니다.\n",
    "            emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "            weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "            \n",
    "            # diagonal matrix\n",
    "            for i in range(vocab_size):\n",
    "                weight_matrix[i][i] = 1\n",
    "                \n",
    "            emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "            \n",
    "            # grad flow \n",
    "            emb_layer.weight.requires_grad = emb_train\n",
    "            \n",
    "            list_emb_layer.append(emb_layer)\n",
    "        return list_emb_layer\n",
    "\n",
    "    def _embed(self, x):\n",
    "        list_embed = list()\n",
    "        for i in range(5):\n",
    "            list_embed.append(self.embedding[i](x[:, :, i]))\n",
    "        x = torch.cat(list_embed, 2)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        A = A.float()\n",
    "        x = self._embed(x)   \n",
    "        \n",
    "        for i, module in enumerate(self.gcn_layers):\n",
    "            x, A = module(x, A)\n",
    "        x = self.readout(x)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_X = sample_dataset.X[0]\n",
    "# sample_A = sample_dataset.A[0]\n",
    "# sample_GConv = GConv(115, 20, False)\n",
    "# print(\"X :\", sample_X.shape)\n",
    "# print(\"A :\", sample_A.shape)\n",
    "# sample_GConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, args, **kwargs):\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X, A)\n",
    "        \n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cnt_iter += 1\n",
    "    return model, list_train_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, args):\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, dataloader, args, **kwargs):\n",
    "\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "\n",
    "    mae = mean_absolute_error(list_y, list_pred_y)\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return mae, std, list_y, list_pred_y\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    model = GCNNet(args)    \n",
    "    model.to(args.device)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    args.best_mae = 10000\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_losses = train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
    "        val_loss = validate(model, partition['val'], criterion, args)\n",
    "        mae, std, true_y, pred_y = test(model, partition['test'], args, **{'epoch':epoch})\n",
    "        \n",
    "        list_train_loss += train_losses\n",
    "        list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "        list_mae.append({'epoch':epoch, 'mae':mae})\n",
    "        list_std.append({'epoch':epoch, 'std':std})\n",
    "        \n",
    "        if args.best_mae > mae or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_mae = mae\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "            \n",
    "\n",
    "    # End of experiments\n",
    "    te = time.time()\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "    args.val_losses = list_val_loss\n",
    "    args.maes = list_mae\n",
    "    args.stds = list_std\n",
    "\n",
    "    return model, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# ==== Embedding Config ==== #\n",
    "args.max_len = 70\n",
    "args.vocab_size = 12 #40\n",
    "args.degree_size = 6\n",
    "args.numH_size = 5\n",
    "args.valence_size = 6\n",
    "args.isarom_size = 2\n",
    "args.emb_train = True\n",
    "\n",
    "\n",
    "# ==== Model Architecture Config ==== #\n",
    "args.in_dim = 64\n",
    "args.out_dim = 256\n",
    "args.molvec_dim = 512\n",
    "args.n_layer = 1\n",
    "args.use_bn = True\n",
    "args.act = 'relu'\n",
    "args.dp_rate = 0.3\n",
    "\n",
    "\n",
    "# ==== Optimizer Config ==== #\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "\n",
    "\n",
    "# ==== Training Config ==== #\n",
    "args.epoch = 300\n",
    "args.batch_size = 256\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.exp_name = 'exp1_lr_stage'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

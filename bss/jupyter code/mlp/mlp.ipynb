{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [00:22:04] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df_train = pd.read_csv(path + '/data/org/train_.csv')\n",
    "    df_test = pd.read_csv(path + '/data/org/valid_.csv')\n",
    "    \n",
    "    df_train = df_train.rename(columns={'Unnamed: 0' : \"idx\"})\n",
    "    df_test = df_test.rename(columns={'Unnamed: 0' : \"idx\"})\n",
    "    \n",
    "    df_all = df_train.append(df_test).reset_index(drop=True)\n",
    "    \n",
    "    return df_all, df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = '/Users/skcc10170/Desktop'\n",
    "df_all, df_train, df_test = load_data(path=CURRENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 컬럼 분류하기\n",
    "먼저 다음과 같이 분류할 수 있습니다.\n",
    "- 스마일코드 (1개 컬럼)\n",
    "    - 화합물의 구조를 문자열로 표기\n",
    "- 분자의 지문 데이터 (1024개씩 3개, 3072개 컬럼)\n",
    "    - ecfp : 1024개 column\n",
    "    - fcfp : 1024개 column\n",
    "    - ptfp : 1024개 column\n",
    "- 분자자체 특성 (4개 컬럼)\n",
    "    - MolWt : 화합물의 분자 질량\n",
    "    - clogp : 분배 계수\n",
    "    - sa_score : 합성 가능성\n",
    "    - qed : 약물 유사성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_cols(df):\n",
    "    cols = df.columns\n",
    "\n",
    "    # smiles code\n",
    "    col_smiles = ['SMILES']\n",
    "\n",
    "    # node-edge level (3 footprints)\n",
    "    col_ecfp = list(cols[cols.str.contains('ecfp_')]) # ecfp 1024개\n",
    "    col_fcfp = list(cols[cols.str.contains('fcfp_')]) # fcfp 1024개\n",
    "    col_ptfp = list(cols[cols.str.contains('ptfp_')]) # ptfp 1024개\n",
    "\n",
    "    # graph level\n",
    "    col_mol = list(cols[-5:-1])\n",
    "\n",
    "    # input cols\n",
    "    col_input = col_ecfp + col_fcfp + col_ptfp + col_mol # col_smiles 제외\n",
    "\n",
    "    # label\n",
    "    col_label = ['label']\n",
    "    \n",
    "    return col_smiles[0], col_ecfp, col_fcfp, col_ptfp, col_mol, col_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = classify_cols(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 mol2graph\n",
    "분자를 그래프로 해석한다면\n",
    "- 그래프(분자)\n",
    "- 노드(원자) -> 노드 feature matrix\n",
    "- 엣지(연결관계) -> 엣지 feature matrix (일단 생략)\n",
    "\n",
    "3457이 제일 쉬움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = df_all['SMILES'].apply(lambda x: Chem.MolFromSmiles(x).GetNumAtoms()).max()\n",
    "LIST_SYMBOLS = list(set.union(*df_all['SMILES'].apply(\n",
    "    lambda x: set([atom.GetSymbol() for atom in Chem.MolFromSmiles(x).GetAtoms()])).values))\n",
    "NUM_ATOM_FEATURES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, ['H', 'Br', 'Na', 'Cl', 'Se', 'C', 'P', 'I', 'S', 'F', 'Si', 'O', 'N'], 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN, LIST_SYMBOLS, NUM_ATOM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_feature(atom):\n",
    "    return np.array(char_to_ix(atom.GetSymbol(), LIST_SYMBOLS) +\n",
    "                    char_to_ix(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    char_to_ix(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(int(atom.GetIsAromatic()), [0, 1]))    # (40, 6, 5, 6, 2)\n",
    "\n",
    "\n",
    "def char_to_ix(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        return [0] # Unknown Atom Token\n",
    "    return [allowable_set.index(x)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol2graph(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    num_atom = mol.GetNumAtoms()\n",
    "    \n",
    "    X = np.zeros((num_atom, NUM_ATOM_FEATURES), dtype=np.uint8)\n",
    "    A = np.zeros((num_atom, num_atom), dtype=np.uint8)\n",
    "\n",
    "    A = Chem.rdmolops.GetAdjacencyMatrix(\n",
    "        mol).astype(np.uint8, copy=False)\n",
    "    A += np.eye(num_atom, dtype=np.uint8)\n",
    "    \n",
    "    for idx, atom in enumerate(mol.GetAtoms()):\n",
    "        feature = atom_feature(atom)\n",
    "        X[idx, :] = feature\n",
    "        \n",
    "    bond_a, bond_b = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        bond_a.append(bond.GetBeginAtomIdx())\n",
    "        bond_b.append(bond.GetBeginAtomIdx())\n",
    "        bond_a.append(bond.GetEndAtomIdx())\n",
    "        bond_b.append(bond.GetEndAtomIdx())\n",
    "    edge_index = [bond_a, bond_b]\n",
    "    \n",
    "    return X, A, edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별 차이 없어서 일단 편하게 그냥\n",
    "cols_mol = cols[4]\n",
    "df_train[cols[4]] = (df_train[cols_mol] / df_all[cols_mol].mean()) / df_all[cols_mol].std()\n",
    "df_test[cols[4]] = (df_test[cols_mol] / df_all[cols_mol].mean()) / df_all[cols_mol].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_input = cols[1]+cols[2]+cols[3]\n",
    "cols_label = cols[-1]\n",
    "X_train, y_train = torch.as_tensor(df_train[cols_input].to_numpy()), torch.as_tensor(df_train[cols_label].to_numpy())\n",
    "X_test, y_test = torch.as_tensor(df_test[cols_input].to_numpy()), torch.as_tensor(df_test[cols_label].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for a specific number(my_seed)\n",
    "my_seed = 2020\n",
    "torch.manual_seed(my_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(my_seed)\n",
    "    # torch.cuda.manual_seed_all(my_seed) # for multi-gpu\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toxicdataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.float()\n",
    "        self.y = y\n",
    "        self.len = X.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Toxicdataset(X_train, y_train)\n",
    "test_dataset = Toxicdataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(cols_input) # 1024 * 3 + 5 = 3077\n",
    "hidden_size1 = 120\n",
    "hidden_size2 = 80\n",
    "hidden_size3 = 10\n",
    "\n",
    "num_classes = 2\n",
    "dropout_probability = 0.25\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size1),\n",
    "            nn.BatchNorm1d(hidden_size1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probability)\n",
    "            )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size1, hidden_size2),\n",
    "            nn.BatchNorm1d(hidden_size2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probability)\n",
    "            )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size2, hidden_size3),\n",
    "            nn.BatchNorm1d(hidden_size3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probability)\n",
    "            )\n",
    "        \n",
    "        # 마지막에 relu 추가하지 않는 이유는 cross-entropy에서 softmax를 사용하기 때문\n",
    "        self.final_layer = nn.Linear(hidden_size3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        out = self.final_layer(out)\n",
    "        # out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, hidden_size3, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=120, bias=True)\n",
       "    (1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=80, bias=True)\n",
       "    (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=10, bias=True)\n",
       "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (final_layer): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379692"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(output, target):\n",
    "    return torch.sum(torch.max(output, dim=1)[1] == target).float() / float(target.shape[0])\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    output_list, y_list = [], []\n",
    "    total_loss = 0.\n",
    "    \n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        output_list.extend(output)\n",
    "        y_list.extend(y)\n",
    "        total_loss += criterion(output, y).data.cpu().numpy()\n",
    "        \n",
    "    output_list = torch.stack(output_list)\n",
    "    y_list = torch.stack(y_list)\n",
    "        \n",
    "    return total_loss / len(test_loader.dataset), Accuracy(output_list, y_list), output_list.data, y_list.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Train Loss: 0.0008, Test Loss: 0.0011, Train ACC: 0.8439, Test ACC: 0.7645 *\n"
     ]
    }
   ],
   "source": [
    "train_loss_arr = []\n",
    "# val_loss_arr = []\n",
    "test_loss_arr = []\n",
    "\n",
    "best_ACC, final_ACC = -999., -999.\n",
    "best_pred, best_y = None, None\n",
    "\n",
    "early_stop, early_stop_max = 0., 5.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_loss = 0.\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Convert numpy arrays to torch tensors\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "    \n",
    "        # Forward Pass\n",
    "        model.train()\n",
    "        outputs = model(batch_X)\n",
    "        train_loss = criterion(outputs, batch_y)\n",
    "        epoch_loss += train_loss.data\n",
    "    \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss_arr.append(epoch_loss / len(train_loader.dataset))\n",
    " \n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        \n",
    "        train_loss, ACC_train, _, _ = evaluate(model, train_loader)\n",
    "        # val_loss, ACC_val, val_pred, val_y = evaluate(model, val_loader)\n",
    "        test_loss, ACC_test, test_pred, test_y = evaluate(model, test_loader)\n",
    "        \n",
    "        # val_loss_arr.append(val_loss)\n",
    "        test_loss_arr.append(test_loss)\n",
    "        \n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Train ACC: {:.4f}, Test ACC: {:.4f} *'.format(epoch, num_epochs, train_loss, test_loss, ACC_train, ACC_test))\n",
    "\n",
    "        \n",
    "    #     if best_ACC < ACC_val:\n",
    "    #         best_ACC = ACC_val\n",
    "    #         best_pred = test_pred\n",
    "    #         best_y = test_y\n",
    "    #         early_stop = 0\n",
    "            \n",
    "    #         final_ACC = ACC_test\n",
    "    #         print('Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}, Train ACC: {:.4f}, Valid ACC: {:.4f} *'.format(epoch, num_epochs, train_loss, val_loss, ACC_train, ACC_val))\n",
    "    #     else:\n",
    "    #         early_stop += 1\n",
    "    #         print('Epoch [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}, Train ACC: {:.4f}, Valid ACC: {:.4f}'.format(epoch, num_epochs, train_loss, val_loss, ACC_train, ACC_val))   \n",
    "\n",
    "    # if early_stop >= early_stop_max:\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

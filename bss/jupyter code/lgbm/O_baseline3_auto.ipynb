{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [21:48:14] Enabling RDKit 2019.09.3 jupyter extensions\n",
      "/Users/skcc10170/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = '/Users/skcc10170/Desktop'\n",
    "\n",
    "df_train = pd.read_csv(CURRENT_PATH + '/data/org/train_.csv')\n",
    "df_valid = pd.read_csv(CURRENT_PATH + '/data/org/valid_.csv')\n",
    "df_test = pd.read_csv(CURRENT_PATH + '/data/org/predict_input.csv')\n",
    "\n",
    "df_train['type'] = 'train'\n",
    "df_valid['type'] = 'valid'\n",
    "df_test['type']  = 'test'\n",
    "\n",
    "df_tot = pd.concat([df_train, df_valid, df_test], sort=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature generating from molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot['myf_NumAtoms']            = df_tot.SMILES.apply(lambda x: Chem.MolFromSmiles(x).GetNumAtoms())\n",
    "df_tot['myf_NumHeavyAtoms']       = df_tot.SMILES.apply(lambda x: Chem.MolFromSmiles(x).GetNumHeavyAtoms())\n",
    "df_tot['myf_GetNumBonds']         = df_tot.SMILES.apply(lambda x: Chem.MolFromSmiles(x).GetNumBonds())\n",
    "df_tot['myf_GetNumHeavyBonds']    = df_tot.SMILES.apply(lambda x: Chem.MolFromSmiles(x).GetNumBonds(onlyHeavy=True))\n",
    "\n",
    "df_tot['myf_ExactMolWt']          = df_tot.SMILES.apply(lambda x: Descriptors.ExactMolWt(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_HeavyAtomMolWt']      = df_tot.SMILES.apply(lambda x: Descriptors.HeavyAtomMolWt(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_FpDensityMorgan1']    = df_tot.SMILES.apply(lambda x: Descriptors.FpDensityMorgan1(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_FpDensityMorgan2']    = df_tot.SMILES.apply(lambda x: Descriptors.FpDensityMorgan2(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_FpDensityMorgan3']    = df_tot.SMILES.apply(lambda x: Descriptors.FpDensityMorgan3(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_MaxAbsPartialCharge'] = df_tot.SMILES.apply(lambda x: Descriptors.MaxAbsPartialCharge(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_MaxPartialCharge']    = df_tot.SMILES.apply(lambda x: Descriptors.MaxPartialCharge(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_MinAbsPartialCharge'] = df_tot.SMILES.apply(lambda x: Descriptors.MinAbsPartialCharge(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_MinPartialCharge']    = df_tot.SMILES.apply(lambda x: Descriptors.MinPartialCharge(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_MolWt']               = df_tot.SMILES.apply(lambda x: Descriptors.MolWt(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_NumRadicalElectrons'] = df_tot.SMILES.apply(lambda x: Descriptors.NumRadicalElectrons(Chem.MolFromSmiles(x)))\n",
    "df_tot['myf_NumValenceElectrons'] = df_tot.SMILES.apply(lambda x: Descriptors.NumValenceElectrons(Chem.MolFromSmiles(x)))\n",
    "\n",
    "df_tot['myf_NumDoubleBondType'] = df_tot.SMILES.apply(lambda x: len([1 for b in Chem.MolFromSmiles(x).GetBonds() if b.GetBondTypeAsDouble() == 1.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all columns\n",
    "cols = df_train.columns\n",
    "\n",
    "# smiles code\n",
    "cols_smiles = 'SMILES'\n",
    "\n",
    "# node-edge level (3 footprints)\n",
    "cols_ecfp = list(cols[cols.str.contains('ecfp_')]) # ecfp 1024개\n",
    "cols_fcfp = list(cols[cols.str.contains('fcfp_')]) # fcfp 1024개\n",
    "cols_ptfp = list(cols[cols.str.contains('ptfp_')]) # ptfp 1024개\n",
    "\n",
    "# graph level\n",
    "cols_mol = ['MolWt', 'clogp', 'sa_score', 'qed']\n",
    "\n",
    "### new features\n",
    "cols_new_f = list(cols[cols.str.contains('myf_')])\n",
    "\n",
    "# input cols\n",
    "cols_input1 = cols_ecfp + cols_fcfp + cols_ptfp # don't have to normalize\n",
    "cols_input2 = cols_mol + cols_new_f # have to normalize\n",
    "cols_input  = cols_input1 + cols_input2\n",
    "\n",
    "# label\n",
    "cols_label = 'label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits     = 5\n",
    "random_state = 2020\n",
    "random_seed  = 2020\n",
    "data_random_seed = 2020\n",
    "feature_fraction_seed = 2020\n",
    "\n",
    "num_test     = len(df_test) # 927\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df_train[cols_input].values, df_train[cols_label].values\n",
    "x_valid, y_valid = df_valid[cols_input].values, df_valid[cols_label].values\n",
    "x_test            = df_test[cols_input].values\n",
    "\n",
    "trainset_x = np.vstack([x_train, x_valid])\n",
    "trainset_y = np.hstack([y_train, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_org = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.008,\n",
    "    'num_leaves' : 120,\n",
    "    'feature_fraction': 0.63, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq':1,\n",
    "    'boosting_type' : 'dart',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'max_depth' : 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttrain's binary_logloss: 0.326967\tval's binary_logloss: 0.453842\n",
      "[2000]\ttrain's binary_logloss: 0.236678\tval's binary_logloss: 0.422757\n",
      "Early stopping, best iteration is:\n",
      "[2705]\ttrain's binary_logloss: 0.192705\tval's binary_logloss: 0.413111\n",
      "Fold 0 | Valid Accuracy: 0.8162776780371035, F1 Score: 0.834679590737749\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttrain's binary_logloss: 0.320799\tval's binary_logloss: 0.469229\n",
      "[2000]\ttrain's binary_logloss: 0.232802\tval's binary_logloss: 0.440222\n",
      "[3000]\ttrain's binary_logloss: 0.177855\tval's binary_logloss: 0.429612\n",
      "Early stopping, best iteration is:\n",
      "[3730]\ttrain's binary_logloss: 0.148\tval's binary_logloss: 0.426825\n",
      "Fold 1 | Valid Accuracy: 0.7971274685816876, F1 Score: 0.8132231404958677\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttrain's binary_logloss: 0.323036\tval's binary_logloss: 0.468626\n",
      "[2000]\ttrain's binary_logloss: 0.236758\tval's binary_logloss: 0.438214\n",
      "[3000]\ttrain's binary_logloss: 0.181505\tval's binary_logloss: 0.425288\n",
      "Early stopping, best iteration is:\n",
      "[3620]\ttrain's binary_logloss: 0.15503\tval's binary_logloss: 0.422253\n",
      "Fold 2 | Valid Accuracy: 0.8046734571599761, F1 Score: 0.8218579234972677\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttrain's binary_logloss: 0.330491\tval's binary_logloss: 0.452677\n",
      "[2000]\ttrain's binary_logloss: 0.241759\tval's binary_logloss: 0.423197\n",
      "[3000]\ttrain's binary_logloss: 0.187748\tval's binary_logloss: 0.413567\n",
      "Early stopping, best iteration is:\n",
      "[3611]\ttrain's binary_logloss: 0.162468\tval's binary_logloss: 0.41123\n",
      "Fold 3 | Valid Accuracy: 0.805272618334332, F1 Score: 0.8255501878690286\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[1000]\ttrain's binary_logloss: 0.326564\tval's binary_logloss: 0.455577\n",
      "[2000]\ttrain's binary_logloss: 0.23807\tval's binary_logloss: 0.426609\n",
      "[3000]\ttrain's binary_logloss: 0.182138\tval's binary_logloss: 0.416802\n",
      "Early stopping, best iteration is:\n",
      "[3750]\ttrain's binary_logloss: 0.151128\tval's binary_logloss: 0.413139\n",
      "Fold 4 | Valid Accuracy: 0.8184541641701618, F1 Score: 0.8371843095110155\n",
      "Valid Accuracy: 0.8083610772566521, F1 Score: 0.8264990304221858\n",
      "model_acc_0.8084_f1_0.8265_loss_0.4173\n"
     ]
    }
   ],
   "source": [
    "val_f1, val_acc, val_loss = [], [], []\n",
    "results = np.zeros((kfold.n_splits, num_test), dtype=np.float)\n",
    "prob_results = np.zeros(len(trainset_x), dtype=np.float)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold.split(trainset_x, trainset_y)):\n",
    "\n",
    "    # 데이터셋 나눔(train, valid & x,y)\n",
    "    x_train, y_train = trainset_x[train_idx], trainset_y[train_idx]\n",
    "    x_valid, y_valid = trainset_x[valid_idx], trainset_y[valid_idx]\n",
    "    x_test           = df_test[cols_input].values\n",
    "\n",
    "    # 트레인셋 기준으로 평균값, 표준편차 계산\n",
    "    ## 수치형 변수 기준\n",
    "    ### baseline : cols_input2(수치형)\n",
    "    cktpt = len(cols_input2)\n",
    "    tr_mean, tr_std = x_train[:, -cktpt:].mean(axis=0), x_train[:, -cktpt:].std(axis=0)\n",
    "\n",
    "    # train/valid \n",
    "    x_train[:,-cktpt:] = (x_train[:,-cktpt:] - tr_mean) / (tr_std + 1e-5)\n",
    "    x_valid[:,-cktpt:] = (x_valid[:,-cktpt:] - tr_mean) / (tr_std + 1e-5)\n",
    "    x_test[:,-cktpt:]  = (x_test[:,-cktpt:]  - tr_mean) / (tr_std + 1e-5)\n",
    "\n",
    "    d_train = lgbm.Dataset(x_train, y_train)\n",
    "    d_valid = lgbm.Dataset(x_valid, y_valid)\n",
    "\n",
    "    model = lgbm.train(params, d_train, 30000, valid_sets=[d_valid, d_train], valid_names=['val', 'train'],\n",
    "                       verbose_eval=1000, early_stopping_rounds=100)\n",
    "\n",
    "    valid_pred_prob = model.predict(x_valid, num_iteration=model.best_iteration)\n",
    "    prob_results[valid_idx] = valid_pred_prob\n",
    "    y_valid_pred = (valid_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    f1 = f1_score(y_valid, y_valid_pred)\n",
    "    acc = accuracy_score(y_valid, y_valid_pred)\n",
    "    loss = model.best_score['val']['binary_logloss']\n",
    "\n",
    "    print(f'Fold {i} | Valid Accuracy: {acc}, F1 Score: {f1}')\n",
    "\n",
    "    results[i] = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    val_f1.append(f1)\n",
    "    val_acc.append(acc)\n",
    "    val_loss.append(loss)\n",
    "\n",
    "print(f'Valid Accuracy: {np.mean(val_acc)}, F1 Score: {np.mean(val_f1)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_result = df_tot[['type','SMILES','label']].copy()\n",
    "df_result.loc[:, '1'] = pd.Series(np.hstack([prob_results, results.mean(axis=0)]))\n",
    "df_result.loc[:, 'predict'] = (df_result['1'] > 0.5).astype(int)\n",
    "df_result = df_result[['type', 'SMILES', '1', 'label', 'predict']]\n",
    "\n",
    "OUTPUT_PATH = '/Users/skcc10170/Desktop/data/model_result/'\n",
    "output_name = 'model_acc_' + \"{:.4f}\".format(np.mean(val_acc)) \\\n",
    "                           + '_f1_' + \"{:.4f}\".format(np.mean(val_f1)) \\\n",
    "                           + '_loss_' + \"{:.4f}\".format(np.mean(val_loss))\n",
    "\n",
    "df_result.to_csv(OUTPUT_PATH + output_name + '.csv')\n",
    "# parameterset\n",
    "pd.DataFrame(params).to_json(OUTPUT_PATH + output_name + '.json')\n",
    "print(output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/heartcored98/Standalone-DeepLearning-Chemistry/blob/master/Lec05/Lec05_lipo_graph_gcn_prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install miniconda and rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n!wget https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning-Chemistry/master/Lec04/utils.py -O utils.py\\n!mkdir results\\n!mkdir images\\n\\n!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\\n!chmod +x Miniconda3-latest-Linux-x86_64.sh\\n!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\\n!time conda install -q -y -c conda-forge rdkit\\n\\nimport sys\\nimport os\\nsys.path.append('/usr/local/lib/python3.7/site-packages/')\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!wget https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning-Chemistry/master/Lec04/utils.py -O utils.py\n",
    "!mkdir results\n",
    "!mkdir images\n",
    "\n",
    "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "!time conda install -q -y -c conda-forge rdkit\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Predict Lipophilicity Property Using GCN\n",
    "=====================\n",
    "\n",
    "앞선 실습에서는 SMILES와 CNN 아키텍쳐를 활용한 lipophilicity 예측을 연습해보았습니다. \n",
    "이번 시간에는 분자를 Graph로 표현하여 각 원자의 벡터와 원자 간의 연결 상태를 input으로 다룰 수 있는 GCN을 구현해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset과 DataLoader 준비하기  \n",
    "\n",
    "Graph 형태로 데이터를 준비하기 위해서는 임의 분자의 `SMILES` 문자열이 들어왔을 때 이를 `Rdkit` 분자 오브젝트로 바꾸고 각 원자들의 node feature vector와 adjacency matrix를 만들 필요가 있습니다. 이후에는 custom dataset을 만들어 노드 벡터들이 담긴 X, 연결 상태가 담긴 A, Lipophilicity 값 y를 반환해주면 됩니다.  \n",
    "\n",
    "\n",
    "### 1.1 데이터 준비하기\n",
    "앞선 예제에서 사용했던 것처럼 `Lipophilicity.csv`를 다운 받은 후 train, val, test로 분할해줍니다.  \n",
    "GCN의 경우 문자열의 vocabulary set을 만드는 대신 사용할 원자들의 집합을 직접 정하는 방식으로 대체합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [02:37:00] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "# import ray\n",
    "# ray.init(num_cpus=4)\n",
    "# !wget -q \"http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\" -O Lipophilicity.csv\n",
    "\n",
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "# def get_splitted_lipo_dataset(ratios=[0.8, 0.1, 0.1], seed=123):\n",
    "\n",
    "#     raw_data = pd.read_csv('Lipophilicity.csv') # Open original dataset\n",
    "#     smiles = raw_data['smiles']\n",
    "        \n",
    "#     train_val, test = train_test_split(raw_data, test_size=ratios[2], random_state=seed)\n",
    "#     train, val = train_test_split(train_val, test_size=ratios[1]/(ratios[0]+ratios[1]), random_state=seed)\n",
    "    \n",
    "#     return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df_train = pd.read_csv(path + '/dataset/train_.csv')\n",
    "    df_test = pd.read_csv(path + '/dataset/valid_.csv')\n",
    "    \n",
    "    df_train = df_train.rename(columns={'Unnamed: 0' : \"idx\"})\n",
    "    df_test = df_test.rename(columns={'Unnamed: 0' : \"idx\"})\n",
    "    \n",
    "    df_all = df_train.append(df_test).reset_index(drop=True)\n",
    "    \n",
    "    return df_all, df_train, df_test\n",
    "\n",
    "CURRENT_PATH = '/Users/skcc10170/Desktop'\n",
    "total, df_train, df_valid = load_data(path=CURRENT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>ecfp_0</th>\n",
       "      <th>ecfp_1</th>\n",
       "      <th>ecfp_2</th>\n",
       "      <th>ecfp_3</th>\n",
       "      <th>ecfp_4</th>\n",
       "      <th>ecfp_5</th>\n",
       "      <th>ecfp_6</th>\n",
       "      <th>ecfp_7</th>\n",
       "      <th>...</th>\n",
       "      <th>ptfp_1019</th>\n",
       "      <th>ptfp_1020</th>\n",
       "      <th>ptfp_1021</th>\n",
       "      <th>ptfp_1022</th>\n",
       "      <th>ptfp_1023</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>clogp</th>\n",
       "      <th>sa_score</th>\n",
       "      <th>qed</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5147</td>\n",
       "      <td>CNC(=O)c1ncn2c1COc3c(CCN4CCN(CC4)c5cccc6nc(C)c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>482.588</td>\n",
       "      <td>3.34562</td>\n",
       "      <td>2.842929</td>\n",
       "      <td>0.470342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5243</td>\n",
       "      <td>CN(C1CCN(Cc2ccc(cc2)C(F)(F)F)CC1)C(=O)Cc3ccc(c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>486.531</td>\n",
       "      <td>3.91350</td>\n",
       "      <td>2.420206</td>\n",
       "      <td>0.581325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4735</td>\n",
       "      <td>COc1cc(ccc1n2cnc(C)c2)c3cn(CC(=O)N(C4CCCCC4)c5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>470.577</td>\n",
       "      <td>4.81372</td>\n",
       "      <td>2.705274</td>\n",
       "      <td>0.387214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6527</td>\n",
       "      <td>CCOC(=O)[C@@H]1CC[C@@H](CC1)N2CC(C2)NC(=O)CNc3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>545.584</td>\n",
       "      <td>2.19700</td>\n",
       "      <td>3.012426</td>\n",
       "      <td>0.484449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5803</td>\n",
       "      <td>COC1(CCOCC1)c2ccc(NC(=O)C3=CC(=O)c4cc(F)cc(c4O...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>506.534</td>\n",
       "      <td>4.24884</td>\n",
       "      <td>3.187803</td>\n",
       "      <td>0.431104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3079 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    idx                                             SMILES  ecfp_0  ecfp_1  \\\n",
       "0  5147  CNC(=O)c1ncn2c1COc3c(CCN4CCN(CC4)c5cccc6nc(C)c...       0       0   \n",
       "1  5243  CN(C1CCN(Cc2ccc(cc2)C(F)(F)F)CC1)C(=O)Cc3ccc(c...       0       0   \n",
       "2  4735  COc1cc(ccc1n2cnc(C)c2)c3cn(CC(=O)N(C4CCCCC4)c5...       0       0   \n",
       "3  6527  CCOC(=O)[C@@H]1CC[C@@H](CC1)N2CC(C2)NC(=O)CNc3...       0       0   \n",
       "4  5803  COC1(CCOCC1)c2ccc(NC(=O)C3=CC(=O)c4cc(F)cc(c4O...       0       0   \n",
       "\n",
       "   ecfp_2  ecfp_3  ecfp_4  ecfp_5  ecfp_6  ecfp_7  ...  ptfp_1019  ptfp_1020  \\\n",
       "0       0       0       0       0       0       0  ...          1          0   \n",
       "1       0       0       0       0       0       0  ...          0          0   \n",
       "2       1       0       1       0       0       0  ...          0          0   \n",
       "3       0       0       0       0       0       0  ...          1          0   \n",
       "4       0       0       0       0       0       0  ...          1          0   \n",
       "\n",
       "   ptfp_1021  ptfp_1022  ptfp_1023    MolWt    clogp  sa_score       qed  \\\n",
       "0          1          1          1  482.588  3.34562  2.842929  0.470342   \n",
       "1          1          1          0  486.531  3.91350  2.420206  0.581325   \n",
       "2          1          1          0  470.577  4.81372  2.705274  0.387214   \n",
       "3          1          1          0  545.584  2.19700  3.012426  0.484449   \n",
       "4          1          1          0  506.534  4.24884  3.187803  0.431104   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 3079 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets = get_splitted_lipo_dataset()\n",
    "smiles = df_train\n",
    "smiles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_atom_symbols(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    symbols = set([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return symbols\n",
    "\n",
    "def get_num_atom(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    num_atom = len([atom.GetSymbol() for atom in mol.GetAtoms()])\n",
    "    return num_atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, ['Br', 'I', 'Si', 'Se', 'Na', 'N', 'S', 'F', 'C', 'O', 'H', 'P', 'Cl'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIST_SYMBOLS = list(set.union(*total['SMILES'].apply(lambda x: get_unique_atom_symbols(x)).values))\n",
    "MAX_LEN = max(total['SMILES'].apply(lambda x: get_num_atom(x)))\n",
    "MAX_LEN, LIST_SYMBOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mol2Graph 구현하기  \n",
    "\n",
    "임의의 `SMILES`가 입력되었을 때 이를 분자로 바꿔봅시다. \n",
    "\n",
    "- 노드 행렬 X를 만들 때 원자의 종류뿐만 아니라 Degree, 붙어 있는 수소 원자의 수, Valence, 아로마틱 여부 등을 함께 넣어줌으로써 추가적인 정보를 제공합니다. \n",
    "\n",
    "- 인접 행렬 A를 만들 때는 `Rdkit`에서 기본적으로 계산해주는 행렬에서 대각 성분에 1을 더함으로써 이후 GCN이 이루어질 때 본인의 노드 정보도 함께 추가될 수 있도록 합니다. \n",
    "\n",
    "- 추가적으로 각 분자들은 임의의 원자 개수를 가지고 있으므로 미리 최대 원자 수를 설정해놓고(본 예시에서는 70개) X, A 행렬을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST_SYMBOLS = ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "#             'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "#             'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "#             'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']\n",
    "\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(char_to_ix(atom.GetSymbol(), LIST_SYMBOLS) +\n",
    "                    char_to_ix(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    char_to_ix(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    char_to_ix(int(atom.GetIsAromatic()), [0, 1]))    # (40, 6, 5, 6, 2)\n",
    "\n",
    "\n",
    "def char_to_ix(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        return [0] # Unknown Atom Token\n",
    "    return [allowable_set.index(x)+1]\n",
    "\n",
    "\n",
    "def mol2graph(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "    X = np.zeros((MAX_LEN, 5), dtype=np.uint8)\n",
    "    A = np.zeros((MAX_LEN, MAX_LEN), dtype=np.uint8)\n",
    "\n",
    "    temp_A = Chem.rdmolops.GetAdjacencyMatrix(mol).astype(np.uint8, copy=False)[:MAX_LEN, :MAX_LEN]\n",
    "    num_atom = temp_A.shape[0]\n",
    "    A[:num_atom, :num_atom] = temp_A + np.eye(temp_A.shape[0], dtype=np.uint8)\n",
    "    \n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        feature = atom_feature(atom)\n",
    "        X[i, :] = feature\n",
    "        if i + 1 >= num_atom: break\n",
    "            \n",
    "    return X, A\n",
    "\n",
    "smiles = \"O=C(NCc1ccccn1)c2ccc(Oc3ccccc3)cc2\"\n",
    "# X, A = mol2graph(smiles, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class gcnDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.smiles = df[\"SMILES\"]\n",
    "        self.exp = df[\"label\"].values\n",
    "                \n",
    "        list_X = list()\n",
    "        list_A = list()\n",
    "        for i, smiles in enumerate(self.smiles):\n",
    "            X, A = mol2graph(smiles)\n",
    "            list_X.append(X)\n",
    "            list_A.append(A)\n",
    "            \n",
    "        self.X = np.array(list_X, dtype=np.uint8)\n",
    "        self.A = np.array(list_A, dtype=np.uint8)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.A[index], self.exp[index]\n",
    "    \n",
    "sample_dataset = gcnDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.gcnDataset at 0x1a19e7f9b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Construction\n",
    "\n",
    "Vanila GCN 기반의 Lipophilicity 예측 아키텍쳐를 구현하여 봅시다. 이를 위해 크게 4가지의 Module을 구현하고 사용합니다.  \n",
    "\n",
    "- **BN1d** X 행렬에 대해서 각 노드 피처 벡터들을 Batch Normalization할 수 있는 module입니다.  \n",
    "- **GConv** X, A를 입력 받아서 인접한 노드들의 정보를 바탕으로 각 노드 벡터를 업데이트하는 module입니다.  \n",
    "- **Readout** GConv를 거친 노드 벡터들로부터 invariant한 molecular vector representation을 만들기 위한 pooling module입니다.  \n",
    "- **GCNNet** 노드 행렬 X를 embedding matrix로 변환한 후 `GConv` 모듈을 통과시키고 `Readout` 모듈로 molvec을 만든 후 이로부터 lipophilicity를 예측하는 module입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class BN1d(nn.Module):\n",
    "    def __init__(self, out_dim, use_bn):\n",
    "        super(BN1d, self).__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        if not self.use_bn:\n",
    "            return  x\n",
    "        origin_shape = x.shape\n",
    "        x = x.view(-1, origin_shape[-1])\n",
    "        x = self.bn(x)\n",
    "        x = self.dp(x)\n",
    "        x = x.view(origin_shape)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, use_bn):\n",
    "        super(GConv, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = BN1d(output_dim, use_bn)\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X, A):\n",
    "        x = self.fc(X)\n",
    "        x = torch.matmul(A, x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.relu(x)\n",
    "        return x, A\n",
    "        \n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    def __init__(self, out_dim, molvec_dim):\n",
    "        super(Readout, self).__init__()\n",
    "        self.readout_fc = nn.Linear(out_dim, molvec_dim)\n",
    "        nn.init.xavier_normal_(self.readout_fc.weight.data)\n",
    "\n",
    "    def forward(self, output_H):\n",
    "        molvec = self.readout_fc(output_H)\n",
    "        molvec = torch.mean(molvec, dim=1)\n",
    "        return molvec\n",
    "    \n",
    "\n",
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        # Create Atom Element embedding layer\n",
    "        self.embedding = self.create_emb_layer([args.vocab_size, args.degree_size,\n",
    "                                                args.numH_size, args.valence_size,\n",
    "                                                args.isarom_size],  args.emb_train)    \n",
    "        \n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        for i in range(args.n_layer):\n",
    "            self.gcn_layers.append(GConv(args.in_dim if i==0 else args.out_dim, args.out_dim, args.use_bn))\n",
    "                                   \n",
    "        self.readout = Readout(args.out_dim, args.molvec_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.molvec_dim, args.molvec_dim//2)\n",
    "        self.fc2 = nn.Linear(args.molvec_dim//2, args.molvec_dim//2)\n",
    "        self.fc3 = nn.Linear(args.molvec_dim//2, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def create_emb_layer(self, list_vocab_size, emb_train=False):\n",
    "        list_emb_layer = nn.ModuleList()\n",
    "        for i, vocab_size in enumerate(list_vocab_size):\n",
    "            vocab_size += 1\n",
    "            emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "            weight_matrix = torch.zeros((vocab_size, vocab_size))\n",
    "            for i in range(vocab_size):\n",
    "                weight_matrix[i][i] = 1\n",
    "            emb_layer.load_state_dict({'weight': weight_matrix})\n",
    "            emb_layer.weight.requires_grad = emb_train\n",
    "            list_emb_layer.append(emb_layer)\n",
    "        return list_emb_layer\n",
    "\n",
    "    def _embed(self, x):\n",
    "        list_embed = list()\n",
    "        for i in range(5):\n",
    "            list_embed.append(self.embedding[i](x[:, :, i]))\n",
    "        x = torch.cat(list_embed, 2)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, A):\n",
    "        A = A.float()\n",
    "        x = self._embed(x)   \n",
    "        \n",
    "        for i, module in enumerate(self.gcn_layers):\n",
    "            x, A = module(x, A)\n",
    "        x = self.readout(x)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dp(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dp(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return torch.squeeze(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train, Validation, Test\n",
    "\n",
    "Data, Model, Loss, Optimization을 모두 같이 사용하여 봅시다. Epoch 별로 train과 validation, test가 이루어질 수 있게 함수를 나누었습니다. 이 때 `DataLoader`로부터 X, A, y를 받은 후 `model(X,A)`를 수행하여 `pred_y`를 구한다는 점이 CNN 실습과 다릅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, args, **kwargs):\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    list_train_loss = list()\n",
    "    cnt_iter = 0\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_y = model(X, A)\n",
    "        \n",
    "        train_loss = criterion(pred_y, y)\n",
    "        epoch_train_loss += train_loss.item()\n",
    "        list_train_loss.append({'epoch':batch_idx/len(dataloader)+kwargs['epoch'], 'train_loss':train_loss.item()})\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "        \n",
    "        cnt_iter += 1\n",
    "    \n",
    "    f1 = f1_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    accuracy = accuracy_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return model, list_train_loss, (f1, accuracy, list_y, list_pred_y)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, args):\n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    cnt_iter = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        val_loss = criterion(pred_y, y)\n",
    "        epoch_val_loss += val_loss.item()\n",
    "        cnt_iter += 1\n",
    "\n",
    "    return epoch_val_loss/cnt_iter\n",
    "\n",
    "def test(model, dataloader, args, criterion, **kwargs):\n",
    "\n",
    "    list_y, list_pred_y = list(), list()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X, A, y = batch[0].long(), batch[1].long(), batch[2].float()\n",
    "        X, A, y = X.to(args.device), A.to(args.device), y.to(args.device)\n",
    "    \n",
    "        model.eval()\n",
    "        pred_y = model(X, A)\n",
    "        list_y += y.cpu().detach().numpy().tolist()\n",
    "        list_pred_y += pred_y.cpu().detach().numpy().tolist()\n",
    "\n",
    "    f1 = f1_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    accuracy = accuracy_score(list_y, np.around(np.array(list_pred_y)).tolist())\n",
    "    std = np.std(np.array(list_y)-np.array(list_pred_y))\n",
    "    return f1, accuracy, std, list_y, list_pred_y\n",
    "\n",
    "\n",
    "def experiment(cnt_exp, partition, args):\n",
    "    ts = time.time()\n",
    "    \n",
    "    model = GCNNet(args)    \n",
    "    model.to(args.device)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    if args.optim == 'ADAM':\n",
    "        optimizer = optim.Adam(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'RMSProp':\n",
    "        optimizer = optim.RMSprop(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    elif args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(trainable_parameters, lr=args.lr, weight_decay=args.l2_coef)\n",
    "    else:\n",
    "        assert False, \"Undefined Optimizer Type\"\n",
    "        \n",
    "    # Train, Validate, Evaluate\n",
    "    list_train_loss = list()\n",
    "    list_val_loss = list()\n",
    "    list_mae = list()\n",
    "    list_std = list()\n",
    "    \n",
    "    args.best_valid_loss = 1000\n",
    "    for epoch in range(args.epoch):\n",
    "        model, train_losses, train_results = train(model, partition['train'], optimizer, criterion, args, **{'epoch':epoch})\n",
    "#         val_loss = validate(model, partition['val'], criterion, args)\n",
    "        f1_score, accuracy, std, true_y, pred_y = test(model, partition['val'], args, criterion, **{'epoch':epoch})\n",
    "        \n",
    "        list_train_loss += train_losses\n",
    "#         list_val_loss.append({'epoch':epoch, 'val_loss':val_loss})\n",
    "#         list_mae.append({'epoch':epoch, 'f1_score':mae})\n",
    "#         list_std.append({'epoch':epoch, 'std':std})\n",
    "        train_loss = log_loss(train_results[-2], train_results[-1])\n",
    "        valid_loss = log_loss(true_y, pred_y)\n",
    "        \n",
    "        print('[Exp {:2}] Epoch {:2}'.format(cnt_exp,\n",
    "                                               epoch))\n",
    "    \n",
    "        print('Train f1: {:2.3f}, Valid f1: {:2.3f}, Train acc: {:2.3f}, Valid acc: {:2.3f}, Train loss: {:2.3f}, Valid loss: {:2.3f}'.format(\n",
    "                                                    train_results[0],\n",
    "                                                    f1_score, \n",
    "                                                    train_results[1],\n",
    "                                                    accuracy,\n",
    "                                                    train_loss,\n",
    "                                                    valid_loss))\n",
    "        if args.best_valid_loss > valid_loss or epoch==0:\n",
    "            args.best_epoch = epoch\n",
    "            args.best_f1_score = f1_score\n",
    "            args.best_accuracy = accuracy\n",
    "            args.best_std = std\n",
    "            args.best_true_y = true_y\n",
    "            args.best_pred_y = pred_y\n",
    "            args.best_valid_loss = valid_loss\n",
    "            print('-------------------------------\\nBest Valid: got f1_score: {:2.3f}, accuracy: {:2.3f}, at epoch {:2}, loss: {:2.3f}\\n------------------------------- '.format(f1_score, \n",
    "                                                       accuracy, \n",
    "                                                       epoch,\n",
    "                                                       valid_loss))\n",
    "            \n",
    "        \n",
    "    # End of experiments\n",
    "    te = time.time()\n",
    "    args.elapsed = te-ts\n",
    "    args.train_losses = list_train_loss\n",
    "#     args.val_losses = list_val_loss\n",
    "#     args.maes = list_mae\n",
    "#     args.stds = list_std\n",
    "\n",
    "    return model, args "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment\n",
    "\n",
    "실험을 진행해봅시다. 이때 Embedding, Model Architecture, Optimizer, Training Configuration을 설정할 필요가 있습니다.  \n",
    "첫번째 실험으로 Learning Rate와 N Layer를 바꿔가면서 실험해보도록 하겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp  0] Epoch  0\n",
      "Train f1: 0.660, Valid f1: 0.707, Train acc: 0.588, Valid acc: 0.602, Train loss: 0.667, Valid loss: 0.772\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.707, accuracy: 0.602, at epoch  0, loss: 0.772\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  1\n",
      "Train f1: 0.699, Valid f1: 0.710, Train acc: 0.635, Valid acc: 0.651, Train loss: 0.640, Valid loss: 0.638\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.710, accuracy: 0.651, at epoch  1, loss: 0.638\n",
      "------------------------------- \n",
      "[Exp  0] Epoch  2\n",
      "Train f1: 0.703, Valid f1: 0.712, Train acc: 0.656, Valid acc: 0.589, Train loss: 0.631, Valid loss: 0.754\n",
      "[Exp  0] Epoch  3\n",
      "Train f1: 0.709, Valid f1: 0.728, Train acc: 0.664, Valid acc: 0.646, Train loss: 0.619, Valid loss: 0.632\n",
      "-------------------------------\n",
      "Best Valid: got f1_score: 0.728, accuracy: 0.646, at epoch  3, loss: 0.632\n",
      "------------------------------- \n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time \n",
    "from sklearn.metrics import mean_absolute_error, f1_score, accuracy_score, log_loss\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "# ==== Embedding Config ==== #\n",
    "args.max_len = 88 #MAX_LEN\n",
    "args.vocab_size = 40\n",
    "args.degree_size = 6\n",
    "args.numH_size = 5\n",
    "args.valence_size = 6\n",
    "args.isarom_size = 2\n",
    "args.emb_train = True\n",
    "\n",
    "\n",
    "# ==== Model Architecture Config ==== #\n",
    "args.in_dim = 64\n",
    "args.out_dim = 256\n",
    "args.molvec_dim = 512\n",
    "args.n_layer = 1\n",
    "args.use_bn = True\n",
    "args.act = 'relu'\n",
    "args.dp_rate = 0.5\n",
    "\n",
    "\n",
    "# ==== Optimizer Config ==== #\n",
    "args.lr = 0.00005\n",
    "args.l2_coef = 0.0001\n",
    "args.optim = 'ADAM'\n",
    "\n",
    "\n",
    "# ==== Training Config ==== #\n",
    "args.epoch = 1000\n",
    "args.batch_size = 256\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args.exp_name = 'exp1_lr_stage'\n",
    "\n",
    "\n",
    "# writer = Writer(prior_keyword=['n_layer', 'use_bn', 'lr', 'dp_rate', 'emb_train', 'epoch', 'batch_size'])\n",
    "# writer.clear()\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "#list_n_layer = [1]\n",
    "list_lr = [0.002, 0.005]\n",
    "list_n_layer = [10,4,5]\n",
    "\n",
    "train_dataloader = DataLoader(gcnDataset(df_train), batch_size=args.batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(gcnDataset(df_valid), batch_size=args.batch_size, shuffle=False)\n",
    "# test_dataloader = DataLoader(gcnDataset(datasets[2], args.max_len), batch_size=args.batch_size, shuffle=False)\n",
    "partition = {'train': train_dataloader, 'val': val_dataloader }#, 'test': test_dataloader}\n",
    "\n",
    "cnt_exp = 0\n",
    "for lr in list_lr:\n",
    "    for n_layer in list_n_layer:\n",
    "        args.lr = lr\n",
    "        args.n_layer = n_layer\n",
    "\n",
    "        model, result = experiment(cnt_exp, partition, args)\n",
    "        writer.write(result)\n",
    "        \n",
    "        cnt_exp += 1\n",
    "        print('[Exp {:2}] got f1_score: {:2.3f}, accuracy: {:2.3f}, std: {:2.3f}, valid_loss{:2.3f} at epoch {:2} took {:3.1f} sec'.format(cnt_exp, result.best_f1_score, result.best_accuracy, result.best_std, result.best_valid_loss, result.best_epoch, result.elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "results = writer.read(exp_name='exp1_lr_stage')\n",
    "variable1 = 'n_layer'\n",
    "variable2 = 'lr'\n",
    "\n",
    "\n",
    "plot_performance(results, variable1, variable2, args,\n",
    "                'Performance depends on {} vs {}'.format(variable1, variable2),\n",
    "                'exp1_Performance {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_distribution(results, variable1, variable2, 'true_y', 'pred_y', \n",
    "                  'Prediction results depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Prediction {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plot_loss(results, variable1, variable2, 'epoch', 'loss', \n",
    "                  'Loss depends on {} vs {}'.format(variable1, variable2),\n",
    "                  'exp1_Loss {} vs {}'.format(variable1, variable2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Visualizing Embedding Matrix  \n",
    "\n",
    "`args.emb_train=True`로 하게 되면 one-hot-encoding된 임베딩 weight의 값들도 Loss를 최소화하는 방향으로 학습될 수 있습니다. 실제로 학습된 모델의 embedding matrix를 시각화하여 값이 변화했는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.imshow(model.embedding[0].weight.detach().cpu().numpy(), interpolation='nearest', cmap=plt.cm.Reds)\n",
    "ax.set_yticks(np.arange(len(LIST_SYMBOLS)+1))\n",
    "ax.set_xticks(np.arange(len(LIST_SYMBOLS)+1))\n",
    "\n",
    "ax.set_yticklabels(['PAD'] + LIST_SYMBOLS)\n",
    "ax.set_title(\"Embedding Weight Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
